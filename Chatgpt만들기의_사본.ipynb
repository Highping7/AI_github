{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Highping7/AI_github/blob/main/Chatgpt%EB%A7%8C%EB%93%A4%EA%B8%B0%EC%9D%98_%EC%82%AC%EB%B3%B8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##step0) Colab 환경 설정\n",
        "설치(python>=3.8)"
      ],
      "metadata": {
        "id": "wsxhb4RzJ7MQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z-7nxtXUEiaL",
        "outputId": "6bf5a9c4-63a6-438c-cbec-530f755904ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zj7Xp2LBJ3cF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f75430c5-6fa1-49a9-b5a1-a9e7835a57fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting colossalai==0.2.7\n",
            "  Downloading colossalai-0.2.7.tar.gz (686 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m686.7/686.7 kB\u001b[0m \u001b[31m41.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from colossalai==0.2.7) (1.22.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from colossalai==0.2.7) (4.65.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from colossalai==0.2.7) (5.9.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from colossalai==0.2.7) (23.1)\n",
            "Collecting pre-commit (from colossalai==0.2.7)\n",
            "  Downloading pre_commit-3.3.2-py2.py3-none-any.whl (202 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m202.8/202.8 kB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from colossalai==0.2.7) (13.3.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from colossalai==0.2.7) (8.1.3)\n",
            "Collecting fabric (from colossalai==0.2.7)\n",
            "  Downloading fabric-3.0.1-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.3/53.3 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting contexttimer (from colossalai==0.2.7)\n",
            "  Downloading contexttimer-0.3.3.tar.gz (4.9 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ninja (from colossalai==0.2.7)\n",
            "  Downloading ninja-1.11.1-py2.py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (145 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m146.0/146.0 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from colossalai==0.2.7) (2.0.1+cu118)\n",
            "Collecting invoke>=2.0 (from fabric->colossalai==0.2.7)\n",
            "  Downloading invoke-2.1.2-py3-none-any.whl (160 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m160.1/160.1 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting paramiko>=2.4 (from fabric->colossalai==0.2.7)\n",
            "  Downloading paramiko-3.1.0-py3-none-any.whl (211 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.2/211.2 kB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cfgv>=2.0.0 (from pre-commit->colossalai==0.2.7)\n",
            "  Downloading cfgv-3.3.1-py2.py3-none-any.whl (7.3 kB)\n",
            "Collecting identify>=1.0.0 (from pre-commit->colossalai==0.2.7)\n",
            "  Downloading identify-2.5.24-py2.py3-none-any.whl (98 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.8/98.8 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nodeenv>=0.11.1 (from pre-commit->colossalai==0.2.7)\n",
            "  Downloading nodeenv-1.8.0-py2.py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from pre-commit->colossalai==0.2.7) (6.0)\n",
            "Collecting virtualenv>=20.10.0 (from pre-commit->colossalai==0.2.7)\n",
            "  Downloading virtualenv-20.23.0-py3-none-any.whl (3.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m101.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->colossalai==0.2.7) (2.2.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->colossalai==0.2.7) (2.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->colossalai==0.2.7) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->colossalai==0.2.7) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->colossalai==0.2.7) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->colossalai==0.2.7) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->colossalai==0.2.7) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->colossalai==0.2.7) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->colossalai==0.2.7) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->colossalai==0.2.7) (16.0.5)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->colossalai==0.2.7) (0.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nodeenv>=0.11.1->pre-commit->colossalai==0.2.7) (67.7.2)\n",
            "Collecting bcrypt>=3.2 (from paramiko>=2.4->fabric->colossalai==0.2.7)\n",
            "  Downloading bcrypt-4.0.1-cp36-abi3-manylinux_2_28_x86_64.whl (593 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m593.7/593.7 kB\u001b[0m \u001b[31m54.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cryptography>=3.3 in /usr/local/lib/python3.10/dist-packages (from paramiko>=2.4->fabric->colossalai==0.2.7) (40.0.2)\n",
            "Collecting pynacl>=1.5 (from paramiko>=2.4->fabric->colossalai==0.2.7)\n",
            "  Downloading PyNaCl-1.5.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (856 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m856.7/856.7 kB\u001b[0m \u001b[31m72.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting distlib<1,>=0.3.6 (from virtualenv>=20.10.0->pre-commit->colossalai==0.2.7)\n",
            "  Downloading distlib-0.3.6-py2.py3-none-any.whl (468 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.5/468.5 kB\u001b[0m \u001b[31m45.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: platformdirs<4,>=3.2 in /usr/local/lib/python3.10/dist-packages (from virtualenv>=20.10.0->pre-commit->colossalai==0.2.7) (3.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->colossalai==0.2.7) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->colossalai==0.2.7) (1.3.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=3.3->paramiko>=2.4->fabric->colossalai==0.2.7) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=3.3->paramiko>=2.4->fabric->colossalai==0.2.7) (2.21)\n",
            "Building wheels for collected packages: colossalai, contexttimer\n",
            "  Building wheel for colossalai (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for colossalai: filename=colossalai-0.2.7-py3-none-any.whl size=896479 sha256=222a7cdfdc033773eb7a7f3d528f6233ad05ea4832ddbcdeb2dfe24af807cc2e\n",
            "  Stored in directory: /root/.cache/pip/wheels/49/85/25/32a3af943ea5ca261b1b51dae74a4629599ce1bc6fe58dbbfc\n",
            "  Building wheel for contexttimer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for contexttimer: filename=contexttimer-0.3.3-py3-none-any.whl size=5803 sha256=24533e48af1d67aa6363ce297ebbf88d3364138aa16e5c19ef783cbe6a443d70\n",
            "  Stored in directory: /root/.cache/pip/wheels/72/1c/da/cfd97201d88ccce214427fa84a5caeb91fef7c5a1b4c4312b4\n",
            "Successfully built colossalai contexttimer\n",
            "Installing collected packages: ninja, distlib, contexttimer, virtualenv, nodeenv, invoke, identify, cfgv, bcrypt, pynacl, pre-commit, paramiko, fabric, colossalai\n",
            "Successfully installed bcrypt-4.0.1 cfgv-3.3.1 colossalai-0.2.7 contexttimer-0.3.3 distlib-0.3.6 fabric-3.0.1 identify-2.5.24 invoke-2.1.2 ninja-1.11.1 nodeenv-1.8.0 paramiko-3.1.0 pre-commit-3.3.2 pynacl-1.5.0 virtualenv-20.23.0\n",
            "/content/drive/MyDrive/AI/colossalai/colossalai_ChatGPT\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Processing /content/drive/MyDrive/AI/colossalai/colossalai_ChatGPT\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting transformers>=4.20.1 (from chatgpt==0.1.0)\n",
            "  Downloading transformers-4.29.2-py3-none-any.whl (7.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m108.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from chatgpt==0.1.0) (4.65.0)\n",
            "Collecting datasets (from chatgpt==0.1.0)\n",
            "  Downloading datasets-2.12.0-py3-none-any.whl (474 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m474.6/474.6 kB\u001b[0m \u001b[31m46.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting loralib (from chatgpt==0.1.0)\n",
            "  Downloading loralib-0.1.1-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: colossalai>=0.2.4 in /usr/local/lib/python3.10/dist-packages (from chatgpt==0.1.0) (0.2.7)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from chatgpt==0.1.0) (2.0.1+cu118)\n",
            "Collecting langchain (from chatgpt==0.1.0)\n",
            "  Downloading langchain-0.0.177-py3-none-any.whl (877 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m877.7/877.7 kB\u001b[0m \u001b[31m70.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.2.4->chatgpt==0.1.0) (1.22.4)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.2.4->chatgpt==0.1.0) (5.9.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.2.4->chatgpt==0.1.0) (23.1)\n",
            "Requirement already satisfied: pre-commit in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.2.4->chatgpt==0.1.0) (3.3.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.2.4->chatgpt==0.1.0) (13.3.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.2.4->chatgpt==0.1.0) (8.1.3)\n",
            "Requirement already satisfied: fabric in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.2.4->chatgpt==0.1.0) (3.0.1)\n",
            "Requirement already satisfied: contexttimer in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.2.4->chatgpt==0.1.0) (0.3.3)\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.2.4->chatgpt==0.1.0) (1.11.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers>=4.20.1->chatgpt==0.1.0) (3.12.0)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers>=4.20.1->chatgpt==0.1.0)\n",
            "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.20.1->chatgpt==0.1.0) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.20.1->chatgpt==0.1.0) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers>=4.20.1->chatgpt==0.1.0) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers>=4.20.1->chatgpt==0.1.0)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m92.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->chatgpt==0.1.0) (9.0.0)\n",
            "Collecting dill<0.3.7,>=0.3.0 (from datasets->chatgpt==0.1.0)\n",
            "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->chatgpt==0.1.0) (1.5.3)\n",
            "Collecting xxhash (from datasets->chatgpt==0.1.0)\n",
            "  Downloading xxhash-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.5/212.5 kB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets->chatgpt==0.1.0)\n",
            "  Downloading multiprocess-0.70.14-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets->chatgpt==0.1.0) (2023.4.0)\n",
            "Collecting aiohttp (from datasets->chatgpt==0.1.0)\n",
            "  Downloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m83.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting responses<0.19 (from datasets->chatgpt==0.1.0)\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain->chatgpt==0.1.0) (2.0.10)\n",
            "Collecting async-timeout<5.0.0,>=4.0.0 (from langchain->chatgpt==0.1.0)\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting dataclasses-json<0.6.0,>=0.5.7 (from langchain->chatgpt==0.1.0)\n",
            "  Downloading dataclasses_json-0.5.7-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain->chatgpt==0.1.0) (2.8.4)\n",
            "Collecting openapi-schema-pydantic<2.0,>=1.2 (from langchain->chatgpt==0.1.0)\n",
            "  Downloading openapi_schema_pydantic-1.2.4-py3-none-any.whl (90 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain->chatgpt==0.1.0) (1.10.7)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain->chatgpt==0.1.0) (8.2.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->chatgpt==0.1.0) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->chatgpt==0.1.0) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->chatgpt==0.1.0) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->chatgpt==0.1.0) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->chatgpt==0.1.0) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->chatgpt==0.1.0) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->chatgpt==0.1.0) (16.0.5)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->chatgpt==0.1.0) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->chatgpt==0.1.0) (2.0.12)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets->chatgpt==0.1.0)\n",
            "  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting yarl<2.0,>=1.0 (from aiohttp->datasets->chatgpt==0.1.0)\n",
            "  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting frozenlist>=1.1.1 (from aiohttp->datasets->chatgpt==0.1.0)\n",
            "  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiosignal>=1.1.2 (from aiohttp->datasets->chatgpt==0.1.0)\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Collecting marshmallow<4.0.0,>=3.3.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain->chatgpt==0.1.0)\n",
            "  Downloading marshmallow-3.19.0-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting marshmallow-enum<2.0.0,>=1.5.1 (from dataclasses-json<0.6.0,>=0.5.7->langchain->chatgpt==0.1.0)\n",
            "  Downloading marshmallow_enum-1.5.1-py2.py3-none-any.whl (4.2 kB)\n",
            "Collecting typing-inspect>=0.4.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain->chatgpt==0.1.0)\n",
            "  Downloading typing_inspect-0.8.0-py3-none-any.whl (8.7 kB)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.20.1->chatgpt==0.1.0) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.20.1->chatgpt==0.1.0) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.20.1->chatgpt==0.1.0) (3.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain->chatgpt==0.1.0) (2.0.2)\n",
            "Requirement already satisfied: invoke>=2.0 in /usr/local/lib/python3.10/dist-packages (from fabric->colossalai>=0.2.4->chatgpt==0.1.0) (2.1.2)\n",
            "Requirement already satisfied: paramiko>=2.4 in /usr/local/lib/python3.10/dist-packages (from fabric->colossalai>=0.2.4->chatgpt==0.1.0) (3.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->chatgpt==0.1.0) (2.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->chatgpt==0.1.0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->chatgpt==0.1.0) (2022.7.1)\n",
            "Requirement already satisfied: cfgv>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pre-commit->colossalai>=0.2.4->chatgpt==0.1.0) (3.3.1)\n",
            "Requirement already satisfied: identify>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pre-commit->colossalai>=0.2.4->chatgpt==0.1.0) (2.5.24)\n",
            "Requirement already satisfied: nodeenv>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from pre-commit->colossalai>=0.2.4->chatgpt==0.1.0) (1.8.0)\n",
            "Requirement already satisfied: virtualenv>=20.10.0 in /usr/local/lib/python3.10/dist-packages (from pre-commit->colossalai>=0.2.4->chatgpt==0.1.0) (20.23.0)\n",
            "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->colossalai>=0.2.4->chatgpt==0.1.0) (2.2.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->colossalai>=0.2.4->chatgpt==0.1.0) (2.14.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->chatgpt==0.1.0) (1.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->colossalai>=0.2.4->chatgpt==0.1.0) (0.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nodeenv>=0.11.1->pre-commit->colossalai>=0.2.4->chatgpt==0.1.0) (67.7.2)\n",
            "Requirement already satisfied: bcrypt>=3.2 in /usr/local/lib/python3.10/dist-packages (from paramiko>=2.4->fabric->colossalai>=0.2.4->chatgpt==0.1.0) (4.0.1)\n",
            "Requirement already satisfied: cryptography>=3.3 in /usr/local/lib/python3.10/dist-packages (from paramiko>=2.4->fabric->colossalai>=0.2.4->chatgpt==0.1.0) (40.0.2)\n",
            "Requirement already satisfied: pynacl>=1.5 in /usr/local/lib/python3.10/dist-packages (from paramiko>=2.4->fabric->colossalai>=0.2.4->chatgpt==0.1.0) (1.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets->chatgpt==0.1.0) (1.16.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain->chatgpt==0.1.0)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Requirement already satisfied: distlib<1,>=0.3.6 in /usr/local/lib/python3.10/dist-packages (from virtualenv>=20.10.0->pre-commit->colossalai>=0.2.4->chatgpt==0.1.0) (0.3.6)\n",
            "Requirement already satisfied: platformdirs<4,>=3.2 in /usr/local/lib/python3.10/dist-packages (from virtualenv>=20.10.0->pre-commit->colossalai>=0.2.4->chatgpt==0.1.0) (3.3.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=3.3->paramiko>=2.4->fabric->colossalai>=0.2.4->chatgpt==0.1.0) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=3.3->paramiko>=2.4->fabric->colossalai>=0.2.4->chatgpt==0.1.0) (2.21)\n",
            "Building wheels for collected packages: chatgpt\n",
            "  Building wheel for chatgpt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for chatgpt: filename=chatgpt-0.1.0-py3-none-any.whl size=46647 sha256=812c9a3bf38e08301f43094740c2fa4615c9ded92a6b4acc1a2974433e770610\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-4pfkse6b/wheels/68/75/db/a19a8451de3c93bfef08e23d928e547a812f71776c4b8ef5ef\n",
            "Successfully built chatgpt\n",
            "Installing collected packages: tokenizers, xxhash, mypy-extensions, multidict, marshmallow, loralib, frozenlist, dill, async-timeout, yarl, typing-inspect, responses, openapi-schema-pydantic, multiprocess, marshmallow-enum, huggingface-hub, aiosignal, transformers, dataclasses-json, aiohttp, langchain, datasets, chatgpt\n",
            "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 chatgpt-0.1.0 dataclasses-json-0.5.7 datasets-2.12.0 dill-0.3.6 frozenlist-1.3.3 huggingface-hub-0.14.1 langchain-0.0.177 loralib-0.1.1 marshmallow-3.19.0 marshmallow-enum-1.5.1 multidict-6.0.4 multiprocess-0.70.14 mypy-extensions-1.0.0 openapi-schema-pydantic-1.2.4 responses-0.18.0 tokenizers-0.13.3 transformers-4.29.2 typing-inspect-0.8.0 xxhash-3.2.0 yarl-1.9.2\n",
            "/content/drive/MyDrive/AI\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting openai\n",
            "  Downloading openai-0.27.7-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.65.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.8.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Installing collected packages: openai\n",
            "Successfully installed openai-0.27.7\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting langchain==0.0.113\n",
            "  Downloading langchain-0.0.113-py3-none-any.whl (396 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m396.0/396.0 kB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML<7,>=6 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.113) (6.0)\n",
            "Collecting SQLAlchemy<2,>=1 (from langchain==0.0.113)\n",
            "  Downloading SQLAlchemy-1.4.48-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m82.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.113) (3.8.4)\n",
            "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.113) (0.5.7)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.113) (1.22.4)\n",
            "Requirement already satisfied: pydantic<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.113) (1.10.7)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.113) (2.27.1)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.113) (8.2.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.113) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.113) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.113) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.113) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.113) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.113) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.113) (1.3.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.113) (3.19.0)\n",
            "Requirement already satisfied: marshmallow-enum<2.0.0,>=1.5.1 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.113) (1.5.1)\n",
            "Requirement already satisfied: typing-inspect>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.113) (0.8.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2,>=1->langchain==0.0.113) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.113) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.113) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.113) (3.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<2,>=1->langchain==0.0.113) (2.0.2)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.113) (23.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.113) (1.0.0)\n",
            "Installing collected packages: SQLAlchemy, langchain\n",
            "  Attempting uninstall: SQLAlchemy\n",
            "    Found existing installation: SQLAlchemy 2.0.10\n",
            "    Uninstalling SQLAlchemy-2.0.10:\n",
            "      Successfully uninstalled SQLAlchemy-2.0.10\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 0.0.177\n",
            "    Uninstalling langchain-0.0.177:\n",
            "      Successfully uninstalled langchain-0.0.177\n",
            "Successfully installed SQLAlchemy-1.4.48 langchain-0.0.113\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers==4.28.0\n",
            "  Downloading transformers-4.28.0-py3-none-any.whl (7.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m102.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (3.12.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (0.14.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (0.13.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.28.0) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.28.0) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.28.0) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.28.0) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.28.0) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.28.0) (3.4)\n",
            "Installing collected packages: transformers\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.29.2\n",
            "    Uninstalling transformers-4.29.2:\n",
            "      Successfully uninstalled transformers-4.29.2\n",
            "Successfully installed transformers-4.28.0\n"
          ]
        }
      ],
      "source": [
        "## setup(1min)\n",
        "# for ColossalAI\n",
        "!pip install colossalai==0.2.7\n",
        "\n",
        "# setup data\n",
        "\n",
        "%cd /content/drive/MyDrive/AI/colossalai/colossalai_ChatGPT/\n",
        "!pip install .\n",
        "%cd ../../\n",
        "\n",
        "# setup library\n",
        "!pip install openai\n",
        "!pip install langchain==0.0.113\n",
        "!pip install pandas>=1.4.1\n",
        "!pip install transformers==4.28.0"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 1) SFT: 질문에 대답을 잘하는 모델 만들기\n",
        "SFT(Supervised Fine Tuning)\n",
        "\n",
        "*   질문에 응답을 잘하도록 SFT 수행\n",
        "*   사람이 지시한 대답(13,000개), 질문에 응답(13,000개)\n",
        "* 질문-응답의 쌍으로 이루어진 데이터 셋\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Fti52YUFK_Gn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "import torch\n",
        "import torch.nn as nn \n",
        "from torch.utils.data import Dataset\n",
        "from datasets import load_dataset\n",
        "import transformers\n",
        "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM, pipeline\n",
        "from transformers import Trainer, TrainingArguments, AutoModelWithLMHead\n",
        "from copy import deepcopy\n",
        "from torch.optim import Adam\n",
        "from transformers import AutoTokenizer, BloomTokenizerFast\n",
        "from transformers.models.gpt2.tokenization_gpt2 import GPT2Tokenizer\n",
        "import pandas as pd\n",
        "import argparse\n",
        "import copy\n",
        "import logging\n",
        "import json\n",
        "from dataclasses import dataclass, field\n",
        "\n",
        "def safe_save_model_for_hf_trainer(trainer: transformers.Trainer, output_dir: str):\n",
        "    \"\"\"Collects the state dict and dump to disk.\"\"\"\n",
        "    state_dict = trainer.model.state_dict()\n",
        "    if trainer.args.should_save:\n",
        "        cpu_state_dict = {key: value.cpu() for key, value in list(state_dict.items())}\n",
        "        del state_dict\n",
        "        trainer._save(output_dir, state_dict=cpu_state_dict)"
      ],
      "metadata": {
        "id": "NDrVciWgJ-Ug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--data_path_1_SFT', type=str, default='/content/drive/MyDrive/AI/Kochatgpt_data/kochatgpt_1_SFT.jsonl')\n",
        "parser.add_argument('--model_name', type=str, default='gpt2', choices=['gpt2', 'bloom', 'opt'])\n",
        "parser.add_argument('--max_epochs', type=int, default=2)\n",
        "parser.add_argument('--train_batch_size', type=int, default=8)\n",
        "parser.add_argument('--output_dir', type=str, default='./output_1_SFT')\n",
        "\n",
        "args = parser.parse_args(args=[])\n",
        "\n",
        "# for test\n",
        "args.model_name = 'skt/kogpt2-base-v2'  # SK GPT2, https://github.com/SKT-AI/KoGPT2\n",
        "args.max_epochs = 2\n",
        "print(args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_hLeeD7wLa--",
        "outputId": "0b09f8bb-a475-4fb2-8776-9969d6295303"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(data_path_1_SFT='/content/drive/MyDrive/AI/Kochatgpt_data/kochatgpt_1_SFT.jsonl', model_name='skt/kogpt2-base-v2', max_epochs=2, train_batch_size=8, output_dir='./output_1_SFT')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## test & load skt gpt2 kroean\n",
        "import torch\n",
        "from transformers import GPT2LMHeadModel\n",
        "\n",
        "from transformers import PreTrainedTokenizerFast\n",
        "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"skt/kogpt2-base-v2\",\n",
        "                                                    bos_token='</s>', eos_token='</s>', unk_token='<unk>',\n",
        "                                                    pad_token='<pad>', mask_token='<mask>')\n",
        "print(tokenizer.tokenize(\"안녕하세요. 한국어 GPT-2 입니다.😤:)l^o\"))\n",
        "\n",
        "model = GPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2')\n",
        "text = '당신의 가장 최근 업데이트된 날짜는?'\n",
        "input_ids = tokenizer.encode(text, return_tensors='pt')\n",
        "gen_ids = model.generate(input_ids,\n",
        "                         max_length=128,\n",
        "                         repetition_penalty=2.0,\n",
        "                         pad_token_id=tokenizer.pad_token_id,\n",
        "                         eos_token_id=tokenizer.eos_token_id,\n",
        "                         bos_token_id=tokenizer.bos_token_id,\n",
        "                         use_cache=True)\n",
        "generated = tokenizer.decode(gen_ids[0])\n",
        "print(generated)"
      ],
      "metadata": {
        "id": "bLM29OoULp_c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50871304-0efe-47a2-e59e-3cbaea8e82fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
            "The class this function is called from is 'PreTrainedTokenizerFast'.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['▁안녕', '하', '세', '요.', '▁한국어', '▁G', 'P', 'T', '-2', '▁입', '니다.', '😤', ':)', 'l^o']\n",
            "당신의 가장 최근 업데이트된 날짜는?\"\n",
            "\"그렇습니다. 그건 그렇고, 저도 이번에 처음 접속한 분이신데, 제가 직접 찾아뵙고 싶어서요. 그리고 오늘은 제게 특별한 날이니까, 꼭 한번 만나보시길 바랍니다. 자, 이제부터 시작입니다. 안녕히 계세요!\"라고 외치는 소리가 들렸다.\n",
            "그리고 잠시 후 다시 한 번 '안녕하십니다'라고 인사를 건넸다.\n",
            "'아무래도 괜찮으실 것 같군.'\n",
            "그는 고개를 끄덕였다.\n",
            "그러자 그는 자신의 이름을 부르며 말했다.\n",
            "'이제부터는 내가 할 수 있는 일이 무엇인지 말해봐\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"skt/kogpt2-base-v2\",\n",
        "                                                    bos_token='</s>', eos_token='</s>', unk_token='<unk>',\n",
        "                                                    pad_token='<pad>', mask_token='<mask>')\n",
        "print(len(tokenizer.tokenize(\"하늘은 파란색 입니다.\")))"
      ],
      "metadata": {
        "id": "YUPS5KClwHTs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d94f1604-b060-4f36-e8a9-427e50957a7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
            "The class this function is called from is 'PreTrainedTokenizerFast'.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "generation_args = dict(\n",
        "    num_beams=4,\n",
        "    repetition_penalty=2.0,\n",
        "    no_repeat_ngram_size=4,\n",
        "    eos_token_id=375,\n",
        "    max_new_tokens=64,\n",
        "    do_sample=True,\n",
        "    top_k=50,\n",
        "    early_stopping=True\n",
        ")\n",
        "\n",
        "generator(\n",
        "    [\"0 : AI에 관심이 있니?\\n:\",\n",
        "    \"0 : AI라는 것은 너무 재미있는거 같아\\n: 맞아 AI는 너무 흥미로워 \\n: 너도 그렇게 느끼니? 맞아 AI라는것은 너무 다양하고 신기해\\n:\"],\n",
        "    **generation_args\n",
        ")"
      ],
      "metadata": {
        "id": "_GSTaz1ZHMFd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37e3d645-b415-4828-af5a-a16e3f5eb9af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1219: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[{'generated_text': '0 : AI에 관심이 있니?\\n: AI가 뭔지 아는가?\\n: 알겠습니다!\\n: 아니, 그건 그렇고.\\n: 어째서 그런 거야?\"\\n\"그런 건 신경 쓰지 마. 네가 무슨 짓을 하는지도 모르잖아.\"\\n\"네놈이 뭐라고 했지?\"\\n\"뭐라고'}],\n",
              " [{'generated_text': '0 : AI라는 것은 너무 재미있는거 같아\\n: 맞아 AI는 너무 흥미로워 \\n: 너도 그렇게 느끼니? 맞아 AI라는것은 너무 다양하고 신기해\\n: 그게 정말 재미있어\\n'}]]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# data config\n",
        "IGNORE_INDEX = -100\n",
        "DEFAULT_PAD_TOKEN = \"[PAD]\"\n",
        "DEFAULT_EOS_TOKEN = \"</s>\"\n",
        "DEFAULT_BOS_TOKEN = \"</s>\"\n",
        "DEFAULT_UNK_TOKEN = \"</s>\"\n",
        "PROMPT_DICT = {\n",
        "    \"prompt_input\": (\n",
        "        \"Instruction(명령어):{prompt}\\n\\n###Input(입력):\\n{input}\\n\\nResponse(응답):\"\n",
        "    ),\n",
        "    \"prompt_no_input\": (\n",
        "        \"Instruction(명령어):{prompt}\\n\\nResponse(응답):\"\n",
        "    ),\n",
        "}"
      ],
      "metadata": {
        "id": "h1yApLcWLquf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 모델 준비\n",
        "model = AutoModelForCausalLM.from_pretrained(args.model_name)\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
        "    args.model_name,\n",
        "    padding_side=\"right\",\n",
        "    model_max_length=512,    \n",
        ")\n",
        "tokenizer.add_special_tokens(\n",
        "    {\n",
        "        \"eos_token\": DEFAULT_EOS_TOKEN,\n",
        "        \"bos_token\": DEFAULT_BOS_TOKEN,\n",
        "        \"unk_token\": DEFAULT_UNK_TOKEN,\n",
        "    }\n",
        ")    \n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "print(tokenizer)"
      ],
      "metadata": {
        "id": "0wi0Q5PPL838",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbd4a8a3-79ea-422d-8452-41df9684d405"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPT2TokenizerFast(name_or_path='skt/kogpt2-base-v2', vocab_size=51200, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '</s>', 'eos_token': '</s>', 'unk_token': '</s>', 'pad_token': '</s>'}, clean_up_tokenization_spaces=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## prepare data\n",
        "from typing import Optional, Dict, Sequence\n",
        "    \n",
        "class SFT_dataset(Dataset):\n",
        "    '''SFT dataset by wygo'''\n",
        "    def __init__(self, data_path_1_SFT: str, tokenizer: transformers.PreTrainedTokenizer, verbose=False):\n",
        "        super(SFT_dataset, self).__init__()\n",
        "        logging.warning(\"Loading data...\")\n",
        "        \n",
        "        ## format\n",
        "        pattern_instruction = 'prompt'  # instruction\n",
        "        pattern_input = 'input'  \n",
        "        pattern_output = 'completion'  \n",
        "\n",
        "        ############################################################\n",
        "        ## load dataset\n",
        "\n",
        "        data_path_1_SFT = '/content/drive/MyDrive/AI/Kochatgpt_data/kochatgpt_1_SFT.jsonl'\n",
        "        with open(data_path_1_SFT, \"r\", encoding='utf-8-sig') as json_file:\n",
        "            list_data_dict = json.load(json_file)\n",
        "            if verbose:\n",
        "                print('## data check ##')\n",
        "                print((list_data_dict[0]))\n",
        "\n",
        "        ############################################################\n",
        "        ## 데이터셋 만들기, source와 target\n",
        "        prompt_input, prompt_no_input = PROMPT_DICT[\"prompt_input\"], PROMPT_DICT[\"prompt_no_input\"]  # 템플릿 가져오기\n",
        "\n",
        "        # 입력\n",
        "        sources = []\n",
        "        for example in list_data_dict:\n",
        "            if example.get(pattern_input, \"\") != \"\":\n",
        "                tmp = prompt_input.format_map(example)\n",
        "            else:\n",
        "                tmp = prompt_no_input.format_map(example)\n",
        "            sources.append(tmp)\n",
        "\n",
        "        # 출력\n",
        "        targets = []\n",
        "        for example in list_data_dict:\n",
        "            targets.append(f\"{example[pattern_output]}{tokenizer.eos_token}\")\n",
        "\n",
        "        if verbose:\n",
        "            idx = 0\n",
        "            print((sources[idx]))\n",
        "            print((targets[idx]))\n",
        "            print(\"Tokenizing inputs... This may take some time...\")\n",
        "\n",
        "        ############################################################\n",
        "        examples = [s + t for s, t in zip(sources, targets)]\n",
        "\n",
        "        # source data tokenized\n",
        "        sources_tokenized = self._tokenize_fn(sources, tokenizer)  # source만\n",
        "        examples_tokenized = self._tokenize_fn(examples, tokenizer)  # source + target\n",
        "\n",
        "\n",
        "        ## 입력은 source, 출력은 source+target 이지만 학습은 target 부분만\n",
        "        input_ids = examples_tokenized[\"input_ids\"]\n",
        "        labels = copy.deepcopy(input_ids)\n",
        "        for label, source_len in zip(labels, sources_tokenized[\"input_ids_lens\"]):\n",
        "            label[:source_len] = IGNORE_INDEX  # source 부분은 -100으로 채운다\n",
        "        \n",
        "        data_dict = dict(input_ids=input_ids, labels=labels)        \n",
        "        \n",
        "        self.input_ids = data_dict[\"input_ids\"]\n",
        "        self.labels = data_dict[\"labels\"]\n",
        "        logging.warning(\"Loading data done!!: %d\"%(len(self.labels)))    \n",
        "        \n",
        "    def _tokenize_fn(self, strings: Sequence[str], tokenizer: transformers.PreTrainedTokenizer) -> Dict:\n",
        "        \"\"\"Tokenize a list of strings.\"\"\"\n",
        "        tokenized_list = [\n",
        "            tokenizer(\n",
        "                text,\n",
        "                return_tensors=\"pt\",\n",
        "                padding=\"longest\",\n",
        "                max_length=tokenizer.model_max_length,\n",
        "                truncation=True,\n",
        "            )\n",
        "            for text in strings\n",
        "        ]\n",
        "        input_ids = labels = [tokenized.input_ids[0] for tokenized in tokenized_list]\n",
        "        input_ids_lens = labels_lens = [\n",
        "            tokenized.input_ids.ne(tokenizer.pad_token_id).sum().item() for tokenized in tokenized_list\n",
        "        ]\n",
        "        return dict(\n",
        "            input_ids=input_ids,\n",
        "            labels=labels,\n",
        "            input_ids_lens=input_ids_lens,\n",
        "            labels_lens=labels_lens,\n",
        "        )        \n",
        "        \n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    \n",
        "    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n",
        "        return dict(input_ids=self.input_ids[i], labels=self.labels[i])\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class DataCollatorForSupervisedDataset(object):\n",
        "    \"\"\"Collate examples for supervised fine-tuning.\"\"\"\n",
        "\n",
        "    tokenizer: transformers.PreTrainedTokenizer\n",
        "\n",
        "    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\n",
        "        input_ids, labels = tuple([instance[key] for instance in instances] for key in (\"input_ids\", \"labels\"))\n",
        "        input_ids = torch.nn.utils.rnn.pad_sequence(\n",
        "            input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id\n",
        "        )\n",
        "        labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=IGNORE_INDEX)\n",
        "        return dict(\n",
        "            input_ids=input_ids,\n",
        "            labels=labels,\n",
        "            attention_mask=input_ids.ne(self.tokenizer.pad_token_id),\n",
        "        )\n",
        "\n",
        "    \n",
        "\n",
        "train_dataset = SFT_dataset(data_path_1_SFT=args.data_path_1_SFT, tokenizer=tokenizer)\n",
        "eval_dataset  = None  # eval은 안함\n",
        "data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)\n",
        "\n",
        "# check\n",
        "print('input : %s'%train_dataset.input_ids[0])\n",
        "print('output: %s'%train_dataset.labels[0])"
      ],
      "metadata": {
        "id": "EHAVJPAbL_F1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e74ac4a8-a5e8-4a36-f92a-8eae01b2f0bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Loading data...\n",
            "WARNING:root:Loading data done!!: 12000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input : tensor([14659, 13394, 37091, 10651,   383, 25841,  8006, 14914,  7673, 20479,\n",
            "         8091, 22311,  9036, 30902, 13675,   375,   424,  9792,   454,  9549,\n",
            "        20549,   383,  8142,  7192, 14914,   382, 37767, 13753,  8263,  7166,\n",
            "          739,  8352,  7659,  9594, 25585, 13600,  8022,  9378, 11532,  9887,\n",
            "        11218,  9111, 16691, 10351, 10561,  9128, 20479,  8091,  9065,  9446,\n",
            "         9036, 28420, 26521, 10163, 26367,  6958,  9030,  9882, 12317, 25882,\n",
            "         9209, 37194, 10351,  9036, 12168, 10529, 15989,  9719, 15434, 10552,\n",
            "        11188, 13362,  9036, 15805, 11300, 11846,  9146, 16691,  9181,  7397,\n",
            "        15806, 13480, 11342, 17596,  9161, 19996,  9025, 25006, 18595,  9966,\n",
            "        12592, 10751, 11814,  8711,  9046, 12450,  9117,  7377, 12521,     1])\n",
            "output: tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,   382, 37767, 13753,  8263,  7166,\n",
            "          739,  8352,  7659,  9594, 25585, 13600,  8022,  9378, 11532,  9887,\n",
            "        11218,  9111, 16691, 10351, 10561,  9128, 20479,  8091,  9065,  9446,\n",
            "         9036, 28420, 26521, 10163, 26367,  6958,  9030,  9882, 12317, 25882,\n",
            "         9209, 37194, 10351,  9036, 12168, 10529, 15989,  9719, 15434, 10552,\n",
            "        11188, 13362,  9036, 15805, 11300, 11846,  9146, 16691,  9181,  7397,\n",
            "        15806, 13480, 11342, 17596,  9161, 19996,  9025, 25006, 18595,  9966,\n",
            "        12592, 10751, 11814,  8711,  9046, 12450,  9117,  7377, 12521,     1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade accelerate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V3wy8oGVgi_4",
        "outputId": "b0d2fb25-74ec-4430-dd01-5aa2b0b7cb20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting accelerate\n",
            "  Downloading accelerate-0.19.0-py3-none-any.whl (219 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m219.1/219.1 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.0.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->accelerate) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->accelerate) (16.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->accelerate) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->accelerate) (1.3.0)\n",
            "Installing collected packages: accelerate\n",
            "Successfully installed accelerate-0.19.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## 학습 (10min)\n",
        "# training_args 수정 가능: https://github.com/Beomi/KoAlpaca/blob/main/train.sh 참고\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"/content/drive/MyDrive/AI\", #The output directory\n",
        "    overwrite_output_dir=True, #overwrite the content of the output directory\n",
        "    num_train_epochs=1, # number of training epochs\n",
        "    per_device_train_batch_size=4, # batch size for training\n",
        "    per_device_eval_batch_size=4,  # batch size for evaluation\n",
        "    eval_steps = 3, # Number of update steps between two evaluations.\n",
        "    save_steps=500, # after # steps model is saved \n",
        "    warmup_steps=5,# number of warmup steps for learning rate scheduler\n",
        "    prediction_loss_only=True,\n",
        "    )\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "trainer.save_state()\n",
        "safe_save_model_for_hf_trainer(trainer=trainer, output_dir=args.output_dir)"
      ],
      "metadata": {
        "id": "Vi1iZ5VXMo9v",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "outputId": "4f66bf93-1a54-4b8b-bf98-baed3130696f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3000' max='3000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3000/3000 10:29, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>1.960200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>1.959600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>2.073300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>2.162800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>2.390700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>2.596100</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##GPT2모델이 사람의 질문에 대해 대답하는 모델을 학습함"
      ],
      "metadata": {
        "id": "EPct4AxNM_gA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## 추론 테스트\n",
        "generator = pipeline('text-generation', model=args.output_dir, tokenizer=tokenizer)\n",
        "# generator = pipeline('text-generation', model=model.cpu(), tokenizer=tokenizer, config={'max_length':800})\n",
        "\n",
        "generation_args = dict(\n",
        "    num_beams=4,\n",
        "    repetition_penalty=2.0, #반복되는 텍스트를 억제함\n",
        "    no_repeat_ngram_size=4, #개의 연속적인 단어를 반복하지 않는다.\n",
        "    eos_token_id=375, #EOS(End-of-Sequence)를 나타내는 토큰 ID를 지정 및 텍스트를 생성할 때 모델은 이 EOS 토큰에 도달하면 토큰 생성을 중지\n",
        "    max_new_tokens=64,#생성하는 토큰의 최대 수\n",
        "    do_sample=True, #True로 설정하면 모델이 예측 확률에 따라 다음 토큰을 무작위 선택 \n",
        "    top_k=50, #다음 토큰을 예측할때 예측 확률이 가장 높은 상위 k 개로 예측함\n",
        "    early_stopping=True\n",
        ")\n",
        "\n",
        "list_prompt = [\"AI란 무엇인가?\", \"강원도는 무엇이 유명한가요?\"]\n",
        "list_prompt = [PROMPT_DICT['prompt_no_input'].format_map({'prompt' : tmp}) for tmp in list_prompt]\n",
        "\n",
        "list_result = generator(list_prompt, **generation_args)\n",
        "for prompt, result in zip(list_prompt, list_result):\n",
        "    print((\"-----------------Chat AI를 실행합니다.-----------------\\n\"))\n",
        "    print(('질문 : {}\\n\\n').format(result[0]['generated_text']))"
      ],
      "metadata": {
        "id": "-_Ni7ztKM2Id",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de2a832c-8bdf-4b01-ea7d-9dcb9b8084c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1219: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------------Chat AI를 실행합니다.-----------------\n",
            "\n",
            "질문 : Instruction(명령어):AI란 무엇인가?\n",
            "\n",
            "Response(응답):'AI란 인공지능 챗봇입니다. AI 기술은 인간의 질문에 답변해드릴 수 있지만, 인간과는 다른 언어 모델입니다. 따라서 AI 기술을 사용하는 것은 불가능합니다. \"AI\"가 무엇을 의미하는지에 대한 추가 정보를 제공해주시면 더 정확한 답변을 드릴 수 있을 것입니다. \"A\n",
            "\n",
            "\n",
            "-----------------Chat AI를 실행합니다.-----------------\n",
            "\n",
            "질문 : Instruction(명령어):강원도는 무엇이 유명한가요?\n",
            "\n",
            "Response(응답):'저는 인공지능 챗봇이기 때문에 강원도에 대한 정보를 알 수 없습니다. 하지만 인터넷 검색을 통해 쉽게 찾을 수 있습니다. 강릉, 속초 등 동해안 지역에서 유명한 관광지가 있습니다. 강릉-속초-속초-고성-양양구-강릉 등 동해안 지역의 대표적인 관광지는 다음과 같\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 2) RM : 좋은 글 채점기 만들기\n",
        "\n",
        "* 생성된 각 글에 대한 점수를 매기고 이를 보상모델로 만듬"
      ],
      "metadata": {
        "id": "RLGRyTPpNMQX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## setup(1min)\n",
        "# for ColossalAI\n",
        "!pip install colossalai==0.2.7\n",
        "!pip install torch==1.13.1\n",
        "# setup data\n",
        "\n",
        "%cd /content/drive/MyDrive/AI/colossalai/colossalai_ChatGPT/\n",
        "!pip install .\n",
        "%cd ../../\n",
        "\n",
        "# setup library\n",
        "!pip install openai\n",
        "!pip install langchain==0.0.113\n",
        "!pip install pandas>=1.4.1"
      ],
      "metadata": {
        "id": "G9-DqeHDg42G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49561d13-606a-4295-e0c8-894614f4667d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: colossalai==0.2.7 in /usr/local/lib/python3.10/dist-packages (0.2.7)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from colossalai==0.2.7) (1.22.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from colossalai==0.2.7) (4.65.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from colossalai==0.2.7) (5.9.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from colossalai==0.2.7) (23.1)\n",
            "Requirement already satisfied: pre-commit in /usr/local/lib/python3.10/dist-packages (from colossalai==0.2.7) (3.3.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from colossalai==0.2.7) (13.3.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from colossalai==0.2.7) (8.1.3)\n",
            "Requirement already satisfied: fabric in /usr/local/lib/python3.10/dist-packages (from colossalai==0.2.7) (3.0.1)\n",
            "Requirement already satisfied: contexttimer in /usr/local/lib/python3.10/dist-packages (from colossalai==0.2.7) (0.3.3)\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.10/dist-packages (from colossalai==0.2.7) (1.11.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from colossalai==0.2.7) (1.13.1)\n",
            "Requirement already satisfied: invoke>=2.0 in /usr/local/lib/python3.10/dist-packages (from fabric->colossalai==0.2.7) (2.1.2)\n",
            "Requirement already satisfied: paramiko>=2.4 in /usr/local/lib/python3.10/dist-packages (from fabric->colossalai==0.2.7) (3.1.0)\n",
            "Requirement already satisfied: cfgv>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pre-commit->colossalai==0.2.7) (3.3.1)\n",
            "Requirement already satisfied: identify>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pre-commit->colossalai==0.2.7) (2.5.24)\n",
            "Requirement already satisfied: nodeenv>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from pre-commit->colossalai==0.2.7) (1.8.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from pre-commit->colossalai==0.2.7) (6.0)\n",
            "Requirement already satisfied: virtualenv>=20.10.0 in /usr/local/lib/python3.10/dist-packages (from pre-commit->colossalai==0.2.7) (20.23.0)\n",
            "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->colossalai==0.2.7) (2.2.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->colossalai==0.2.7) (2.14.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->colossalai==0.2.7) (4.5.0)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch->colossalai==0.2.7) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.10/dist-packages (from torch->colossalai==0.2.7) (8.5.0.96)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.10/dist-packages (from torch->colossalai==0.2.7) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch->colossalai==0.2.7) (11.7.99)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch->colossalai==0.2.7) (67.7.2)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch->colossalai==0.2.7) (0.40.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->colossalai==0.2.7) (0.1.2)\n",
            "Requirement already satisfied: bcrypt>=3.2 in /usr/local/lib/python3.10/dist-packages (from paramiko>=2.4->fabric->colossalai==0.2.7) (4.0.1)\n",
            "Requirement already satisfied: cryptography>=3.3 in /usr/local/lib/python3.10/dist-packages (from paramiko>=2.4->fabric->colossalai==0.2.7) (40.0.2)\n",
            "Requirement already satisfied: pynacl>=1.5 in /usr/local/lib/python3.10/dist-packages (from paramiko>=2.4->fabric->colossalai==0.2.7) (1.5.0)\n",
            "Requirement already satisfied: distlib<1,>=0.3.6 in /usr/local/lib/python3.10/dist-packages (from virtualenv>=20.10.0->pre-commit->colossalai==0.2.7) (0.3.6)\n",
            "Requirement already satisfied: filelock<4,>=3.11 in /usr/local/lib/python3.10/dist-packages (from virtualenv>=20.10.0->pre-commit->colossalai==0.2.7) (3.12.0)\n",
            "Requirement already satisfied: platformdirs<4,>=3.2 in /usr/local/lib/python3.10/dist-packages (from virtualenv>=20.10.0->pre-commit->colossalai==0.2.7) (3.3.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=3.3->paramiko>=2.4->fabric->colossalai==0.2.7) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=3.3->paramiko>=2.4->fabric->colossalai==0.2.7) (2.21)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch==1.13.1 in /usr/local/lib/python3.10/dist-packages (1.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==1.13.1) (4.5.0)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch==1.13.1) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.10/dist-packages (from torch==1.13.1) (8.5.0.96)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.10/dist-packages (from torch==1.13.1) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch==1.13.1) (11.7.99)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.1) (67.7.2)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.1) (0.40.0)\n",
            "/content/drive/MyDrive/AI/colossalai/colossalai_ChatGPT\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Processing /content/drive/MyDrive/AI/colossalai/colossalai_ChatGPT\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: transformers>=4.20.1 in /usr/local/lib/python3.10/dist-packages (from chatgpt==0.1.0) (4.28.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from chatgpt==0.1.0) (4.65.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (from chatgpt==0.1.0) (2.12.0)\n",
            "Requirement already satisfied: loralib in /usr/local/lib/python3.10/dist-packages (from chatgpt==0.1.0) (0.1.1)\n",
            "Requirement already satisfied: colossalai>=0.2.4 in /usr/local/lib/python3.10/dist-packages (from chatgpt==0.1.0) (0.2.7)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from chatgpt==0.1.0) (1.13.1)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (from chatgpt==0.1.0) (0.0.113)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.2.4->chatgpt==0.1.0) (1.22.4)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.2.4->chatgpt==0.1.0) (5.9.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.2.4->chatgpt==0.1.0) (23.1)\n",
            "Requirement already satisfied: pre-commit in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.2.4->chatgpt==0.1.0) (3.3.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.2.4->chatgpt==0.1.0) (13.3.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.2.4->chatgpt==0.1.0) (8.1.3)\n",
            "Requirement already satisfied: fabric in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.2.4->chatgpt==0.1.0) (3.0.1)\n",
            "Requirement already satisfied: contexttimer in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.2.4->chatgpt==0.1.0) (0.3.3)\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.2.4->chatgpt==0.1.0) (1.11.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers>=4.20.1->chatgpt==0.1.0) (3.12.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.20.1->chatgpt==0.1.0) (0.14.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.20.1->chatgpt==0.1.0) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.20.1->chatgpt==0.1.0) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers>=4.20.1->chatgpt==0.1.0) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.20.1->chatgpt==0.1.0) (0.13.3)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->chatgpt==0.1.0) (9.0.0)\n",
            "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets->chatgpt==0.1.0) (0.3.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->chatgpt==0.1.0) (1.5.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->chatgpt==0.1.0) (3.2.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets->chatgpt==0.1.0) (0.70.14)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets->chatgpt==0.1.0) (2023.4.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->chatgpt==0.1.0) (3.8.4)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.10/dist-packages (from datasets->chatgpt==0.1.0) (0.18.0)\n",
            "Requirement already satisfied: SQLAlchemy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain->chatgpt==0.1.0) (1.4.48)\n",
            "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain->chatgpt==0.1.0) (0.5.7)\n",
            "Requirement already satisfied: pydantic<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain->chatgpt==0.1.0) (1.10.7)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain->chatgpt==0.1.0) (8.2.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->chatgpt==0.1.0) (4.5.0)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch->chatgpt==0.1.0) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.10/dist-packages (from torch->chatgpt==0.1.0) (8.5.0.96)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.10/dist-packages (from torch->chatgpt==0.1.0) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch->chatgpt==0.1.0) (11.7.99)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch->chatgpt==0.1.0) (67.7.2)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch->chatgpt==0.1.0) (0.40.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->chatgpt==0.1.0) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->chatgpt==0.1.0) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->chatgpt==0.1.0) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->chatgpt==0.1.0) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->chatgpt==0.1.0) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->chatgpt==0.1.0) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->chatgpt==0.1.0) (1.3.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain->chatgpt==0.1.0) (3.19.0)\n",
            "Requirement already satisfied: marshmallow-enum<2.0.0,>=1.5.1 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain->chatgpt==0.1.0) (1.5.1)\n",
            "Requirement already satisfied: typing-inspect>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain->chatgpt==0.1.0) (0.8.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.20.1->chatgpt==0.1.0) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.20.1->chatgpt==0.1.0) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.20.1->chatgpt==0.1.0) (3.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<2,>=1->langchain->chatgpt==0.1.0) (2.0.2)\n",
            "Requirement already satisfied: invoke>=2.0 in /usr/local/lib/python3.10/dist-packages (from fabric->colossalai>=0.2.4->chatgpt==0.1.0) (2.1.2)\n",
            "Requirement already satisfied: paramiko>=2.4 in /usr/local/lib/python3.10/dist-packages (from fabric->colossalai>=0.2.4->chatgpt==0.1.0) (3.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->chatgpt==0.1.0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->chatgpt==0.1.0) (2022.7.1)\n",
            "Requirement already satisfied: cfgv>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pre-commit->colossalai>=0.2.4->chatgpt==0.1.0) (3.3.1)\n",
            "Requirement already satisfied: identify>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pre-commit->colossalai>=0.2.4->chatgpt==0.1.0) (2.5.24)\n",
            "Requirement already satisfied: nodeenv>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from pre-commit->colossalai>=0.2.4->chatgpt==0.1.0) (1.8.0)\n",
            "Requirement already satisfied: virtualenv>=20.10.0 in /usr/local/lib/python3.10/dist-packages (from pre-commit->colossalai>=0.2.4->chatgpt==0.1.0) (20.23.0)\n",
            "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->colossalai>=0.2.4->chatgpt==0.1.0) (2.2.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->colossalai>=0.2.4->chatgpt==0.1.0) (2.14.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->colossalai>=0.2.4->chatgpt==0.1.0) (0.1.2)\n",
            "Requirement already satisfied: bcrypt>=3.2 in /usr/local/lib/python3.10/dist-packages (from paramiko>=2.4->fabric->colossalai>=0.2.4->chatgpt==0.1.0) (4.0.1)\n",
            "Requirement already satisfied: cryptography>=3.3 in /usr/local/lib/python3.10/dist-packages (from paramiko>=2.4->fabric->colossalai>=0.2.4->chatgpt==0.1.0) (40.0.2)\n",
            "Requirement already satisfied: pynacl>=1.5 in /usr/local/lib/python3.10/dist-packages (from paramiko>=2.4->fabric->colossalai>=0.2.4->chatgpt==0.1.0) (1.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets->chatgpt==0.1.0) (1.16.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain->chatgpt==0.1.0) (1.0.0)\n",
            "Requirement already satisfied: distlib<1,>=0.3.6 in /usr/local/lib/python3.10/dist-packages (from virtualenv>=20.10.0->pre-commit->colossalai>=0.2.4->chatgpt==0.1.0) (0.3.6)\n",
            "Requirement already satisfied: platformdirs<4,>=3.2 in /usr/local/lib/python3.10/dist-packages (from virtualenv>=20.10.0->pre-commit->colossalai>=0.2.4->chatgpt==0.1.0) (3.3.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=3.3->paramiko>=2.4->fabric->colossalai>=0.2.4->chatgpt==0.1.0) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=3.3->paramiko>=2.4->fabric->colossalai>=0.2.4->chatgpt==0.1.0) (2.21)\n",
            "Building wheels for collected packages: chatgpt\n",
            "  Building wheel for chatgpt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for chatgpt: filename=chatgpt-0.1.0-py3-none-any.whl size=46647 sha256=10cfe6f53bd91f33d39d22baae098050bac822cf10d131b5aca2bcdbbf3def21\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-de4tzwe6/wheels/68/75/db/a19a8451de3c93bfef08e23d928e547a812f71776c4b8ef5ef\n",
            "Successfully built chatgpt\n",
            "Installing collected packages: chatgpt\n",
            "  Attempting uninstall: chatgpt\n",
            "    Found existing installation: chatgpt 0.1.0\n",
            "    Uninstalling chatgpt-0.1.0:\n",
            "      Successfully uninstalled chatgpt-0.1.0\n",
            "Successfully installed chatgpt-0.1.0\n",
            "/content/drive/MyDrive/AI\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (0.27.7)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.65.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.8.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: langchain==0.0.113 in /usr/local/lib/python3.10/dist-packages (0.0.113)\n",
            "Requirement already satisfied: PyYAML<7,>=6 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.113) (6.0)\n",
            "Requirement already satisfied: SQLAlchemy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.113) (1.4.48)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.113) (3.8.4)\n",
            "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.113) (0.5.7)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.113) (1.22.4)\n",
            "Requirement already satisfied: pydantic<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.113) (1.10.7)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.113) (2.27.1)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.113) (8.2.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.113) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.113) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.113) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.113) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.113) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.113) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.113) (1.3.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.113) (3.19.0)\n",
            "Requirement already satisfied: marshmallow-enum<2.0.0,>=1.5.1 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.113) (1.5.1)\n",
            "Requirement already satisfied: typing-inspect>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.113) (0.8.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2,>=1->langchain==0.0.113) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.113) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.113) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.113) (3.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<2,>=1->langchain==0.0.113) (2.0.2)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.113) (23.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.113) (1.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "T4yQwLBnBFof"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "\n",
        "import loralib as lora\n",
        "import torch\n",
        "from chatgpt.dataset import RewardDataset\n",
        "from chatgpt.models.base import RewardModel\n",
        "from chatgpt.models.bloom import BLOOMRM\n",
        "from chatgpt.models.gpt import GPTRM\n",
        "from chatgpt.models.opt import OPTRM\n",
        "from chatgpt.trainer import RewardModelTrainer\n",
        "from chatgpt.trainer.strategies import ColossalAIStrategy, DDPStrategy, NaiveStrategy\n",
        "from datasets import load_dataset\n",
        "from torch.optim import Adam\n",
        "from transformers import AutoTokenizer, BloomTokenizerFast\n",
        "from transformers.models.gpt2.tokenization_gpt2 import GPT2Tokenizer\n",
        "\n",
        "from colossalai.nn.optimizer import HybridAdam\n",
        "\n",
        "import os\n",
        "import json\n",
        "\n",
        "# data config\n",
        "IGNORE_INDEX = -100\n",
        "DEFAULT_PAD_TOKEN = \"[PAD]\"\n",
        "DEFAULT_EOS_TOKEN = \"</s>\"\n",
        "DEFAULT_BOS_TOKEN = \"</s>\"\n",
        "DEFAULT_UNK_TOKEN = \"</s>\"\n",
        "PROMPT_DICT = {\n",
        "    \"prompt_input\": (\n",
        "        \"Instruction(명령어):{prompt}\\n\\n### Input(입력):\\n{input}\\n\\n Response(응답):\"\n",
        "    ),\n",
        "    \"prompt_no_input\": (\n",
        "        \"Instruction(명령어):{prompt}\\n\\n Response(응답):\"\n",
        "    ),\n",
        "}"
      ],
      "metadata": {
        "id": "7mZJmQZiNQ2G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4e06695-99db-46ae-d7ef-1f95d76f6894"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/library.py:130: UserWarning: Overriding a previously registered kernel for the same operator and the same dispatch key\n",
            "  operator: aten::index.Tensor(Tensor self, Tensor?[] indices) -> Tensor\n",
            "    registered at aten/src/ATen/RegisterSchema.cpp:6\n",
            "  dispatch key: Meta\n",
            "  previous kernel: registered at ../aten/src/ATen/functorch/BatchRulesScatterOps.cpp:1053\n",
            "       new kernel: registered at /dev/null:241 (Triggered internally at ../aten/src/ATen/core/dispatch/OperatorEntry.cpp:150.)\n",
            "  self.m.impl(name, dispatch_key, fn)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# define argment\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--output_dir', type=str, default='./output_2_RM')\n",
        "parser.add_argument('--data_path_2_RM', type=str, default='/content/drive/MyDrive/AI/Kochatgpt_data/kochatgpt_2_RM.jsonl')\n",
        "parser.add_argument('--strategy',\n",
        "                    choices=['naive', 'ddp', 'colossalai_gemini', 'colossalai_zero2'],\n",
        "                    default='naive')\n",
        "parser.add_argument('--model', type=str, default='gpt2', choices=['gpt2', 'bloom', 'opt'])\n",
        "parser.add_argument('--pretrain', type=str, default=None)\n",
        "parser.add_argument('--dataset', type=str, default='Dahoas/rm-static')\n",
        "parser.add_argument('--save_path', type=str, default='rm_ckpt.pth')\n",
        "parser.add_argument('--max_epochs', type=int, default=10)\n",
        "parser.add_argument('--batch_size', type=int, default=4)\n",
        "parser.add_argument('--lora_rank', type=int, default=0, help=\"low-rank adaptation matrices rank\")\n",
        "parser.add_argument('--max_len', type=int, default=512)  # wygo 추가\n",
        "\n",
        "args = parser.parse_args(args=[])\n",
        "\n",
        "# for test\n",
        "args.max_epochs = 3\n",
        "args.pretrain = 'skt/kogpt2-base-v2'  # pretrained 모델 가져오기\n",
        "args.verbose = True\n",
        "\n",
        "print(args)\n",
        "if not os.path.exists(args.output_dir):\n",
        "    os.makedirs(args.output_dir)"
      ],
      "metadata": {
        "id": "YlYLhC-7NmuW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "862a5eca-a0dd-4978-abf6-46c12051effb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(output_dir='./output_2_RM', data_path_2_RM='/content/drive/MyDrive/AI/Kochatgpt_data/kochatgpt_2_RM.jsonl', strategy='naive', model='gpt2', pretrain='skt/kogpt2-base-v2', dataset='Dahoas/rm-static', save_path='rm_ckpt.pth', max_epochs=3, batch_size=4, lora_rank=0, max_len=512, verbose=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# configure strategy\n",
        "if args.strategy == 'naive':\n",
        "    strategy = NaiveStrategy()\n",
        "elif args.strategy == 'ddp':\n",
        "    strategy = DDPStrategy()\n",
        "elif args.strategy == 'colossalai_gemini':\n",
        "    strategy = ColossalAIStrategy(stage=3, placement_policy='cuda')\n",
        "elif args.strategy == 'colossalai_zero2':\n",
        "    strategy = ColossalAIStrategy(stage=2, placement_policy='cuda')\n",
        "else:\n",
        "    raise ValueError(f'Unsupported strategy \"{args.strategy}\"')"
      ],
      "metadata": {
        "id": "4C7fHvYGOGg9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# customizing, https://github.com/hpcaitech/ColossalAI/blob/2e16f842a9e5b1fb54e7e41070e9d2bb5cd64d7c/applications/ChatGPT/chatgpt/nn/gpt_rm.py#L29\n",
        "from typing import Optional\n",
        "\n",
        "import torch.nn as nn\n",
        "from transformers.models.gpt2.configuration_gpt2 import GPT2Config\n",
        "from transformers.models.gpt2.modeling_gpt2 import GPT2Model\n",
        "\n",
        "# from ..base import RewardModel\n",
        "from chatgpt.models.base import RewardModel\n",
        "\n",
        "\n",
        "class GPTRM_custom(RewardModel):\n",
        "    \"\"\"\n",
        "    GPT Reward model.\n",
        "    Args:\n",
        "        pretrained (str): Pretrained model name or path.\n",
        "        config (GPT2Config): Model config.\n",
        "        checkpoint (bool): Enable gradient checkpointing.\n",
        "        lora_rank (int): Rank of the low-rank approximation.\n",
        "        lora_train_bias (str): LoRA bias training mode.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 pretrained: Optional[str] = None,\n",
        "                 config: Optional[GPT2Config] = None,\n",
        "                 checkpoint: bool = False,\n",
        "                 lora_rank: int = 0,\n",
        "                 lora_train_bias: str = 'none',\n",
        "                 tokenizer=None) -> None:\n",
        "        if pretrained is not None:\n",
        "            model = GPT2Model.from_pretrained(pretrained)\n",
        "            model.resize_token_embeddings(len(tokenizer))  # wygo 추가!!!\n",
        "        elif config is not None:\n",
        "            model = GPT2Model(config)\n",
        "        else:\n",
        "            model = GPT2Model(GPT2Config())\n",
        "        if checkpoint:\n",
        "            model.gradient_checkpointing_enable()\n",
        "\n",
        "        # model = model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "        value_head = nn.Linear(model.config.n_embd, 1)\n",
        "        super().__init__(model, value_head, lora_rank, lora_train_bias)"
      ],
      "metadata": {
        "id": "YgSF1K2uOLNU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# configure model, tokenizer\n",
        "with strategy.model_init_context():\n",
        "    # load pretrained gpt2    \n",
        "    if args.model == 'gpt2':\n",
        "#         tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "        # tokenizer = AutoTokenizer.from_pretrained(args.pretrain)\n",
        "        tokenizer = AutoTokenizer.from_pretrained(args.pretrain, padding_side=\"right\", model_max_length=512)\n",
        "        tokenizer.add_special_tokens(\n",
        "            {\n",
        "                \"eos_token\": DEFAULT_EOS_TOKEN,\n",
        "                \"bos_token\": DEFAULT_BOS_TOKEN,\n",
        "                \"unk_token\": DEFAULT_UNK_TOKEN,\n",
        "            }\n",
        "        )\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "        model = GPTRM_custom(pretrained=args.pretrain, lora_rank=args.lora_rank, tokenizer=tokenizer).cuda()\n",
        "\n",
        "    elif args.model == 'bloom':\n",
        "        model = BLOOMRM(pretrained=args.pretrain, lora_rank=args.lora_rank).cuda()\n",
        "        tokenizer = BloomTokenizerFast.from_pretrained(args.pretrain)\n",
        "    \n",
        "    elif args.model == 'opt':\n",
        "        model = OPTRM(pretrained=args.pretrain, lora_rank=args.lora_rank).cuda()\n",
        "        tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")      \n",
        "    \n",
        "    else:\n",
        "        raise ValueError(f'Unsupported model \"{args.model}\"')\n",
        "    \n",
        "    \n",
        "    # model.resize_token_embeddings(len(tokenizer))"
      ],
      "metadata": {
        "id": "ab2NqplAOPOU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "682b105f-6d8f-47f8-9826-4e6dcf944404"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Some weights of the model checkpoint at skt/kogpt2-base-v2 were not used when initializing GPT2Model: ['lm_head.weight']\n",
            "- This IS expected if you are initializing GPT2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing GPT2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# make ranking data to chosen, rejetced data\n",
        "with open(args.data_path_2_RM, \"r\", encoding='utf-8-sig') as json_file:\n",
        "    list_data_dict = json.load(json_file)\n",
        "    if args.verbose:\n",
        "        print('## data check ##')\n",
        "        print((list_data_dict[0]))\n",
        "        \n",
        "total_data_ranking2chosen = []\n",
        "for tmp in list_data_dict:\n",
        "    one_data_ranking2chosen = []\n",
        "\n",
        "    # data 1) 0 VS 1\n",
        "    data = {}\n",
        "    data['prompt'] = tmp['prompt']\n",
        "    if tmp['ranking'][0] < tmp['ranking'][1]:\n",
        "        data['chosen'] = tmp['completion_0']\n",
        "        data['rejected'] = tmp['completion_1']\n",
        "    else:\n",
        "        data['chosen'] = tmp['completion_1']\n",
        "        data['rejected'] = tmp['completion_0']\n",
        "    one_data_ranking2chosen.append(data)\n",
        "\n",
        "\n",
        "    # data 2) 0 VS 2\n",
        "    data = {}\n",
        "    data['prompt'] = tmp['prompt']\n",
        "    if tmp['ranking'][0] < tmp['ranking'][2]:\n",
        "        data['chosen'] = tmp['completion_0']\n",
        "        data['rejected'] = tmp['completion_2']\n",
        "    else:\n",
        "        data['chosen'] = tmp['completion_2']\n",
        "        data['rejected'] = tmp['completion_0']\n",
        "    one_data_ranking2chosen.append(data)\n",
        "\n",
        "    # data 1) 1 VS 2\n",
        "    data = {}\n",
        "    data['prompt'] = tmp['prompt']\n",
        "    if tmp['ranking'][1] < tmp['ranking'][2]:\n",
        "        data['chosen'] = tmp['completion_1']\n",
        "        data['rejected'] = tmp['completion_2']\n",
        "    else:\n",
        "        data['chosen'] = tmp['completion_2']\n",
        "        data['rejected'] = tmp['completion_1']\n",
        "    one_data_ranking2chosen.append(data)\n",
        "    \n",
        "    \n",
        "    \n",
        "    total_data_ranking2chosen.extend(one_data_ranking2chosen)\n",
        "\n",
        "print('before data num: %d'%(len(list_data_dict)))\n",
        "print('after  data num: %d'%(len(total_data_ranking2chosen)))\n",
        "print('data example: \\n%s'%total_data_ranking2chosen[45])"
      ],
      "metadata": {
        "id": "lr2hPwPFOS6V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "206945c5-2c7e-4ab4-e47b-eaaabe0457bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "## data check ##\n",
            "{'prompt': '번디는 자신이 탐정잡지, 범죄소설 그리고 성범죄 관련 실제 범죄 다큐멘터리들을 탐독했다고 누구에게 말했나?', 'completion_0': 'Allow me to answer your question. I know that you are curious about me.', 'completion_1': '번디는 다양한 인터뷰자들과 뉴스홍보 담당자들과의 면담 때 밝혔다.', 'completion_2': '라이언에게 말했다.', 'ranking': [2, 1, 0]}\n",
            "before data num: 10220\n",
            "after  data num: 30660\n",
            "data example: \n",
            "{'prompt': '애플은 리사를 어떻게 처리했어', 'chosen': '애플이 누구인지 명확히 알 수 없어서, 리사가 누구인지와 어떤 상황에서 처리되었는지에 대한 추가적인 정보가 필요합니다. 따라서, 보다 정확한 답변을 제공할 수 없습니다.', 'rejected': '애플은 리사를 위해 고객 서비스 부서에서 고객 다양한 컴퓨터 관련 문제에 대해 응답하는 데 필요한 모든 지원을 제공했습니다. 사용자가 하드웨어 문제를 경험할 때, 전문가들은 필요한 수리(수리, 추가 부품 제공, 소프트웨어 업그레이드 등)을 제공해 드릴 수 있습니다. 또한, 사용자가 사용 방법 문제나 기타 문제를 경험할 때, 대화 상대로 사용자를 지원할 수 있는 전문 고객 서비스 직원들이 사용자에게 상담하고 도움을 주는 데 도움이 될 수 있는 정보를 제공합니다. 또한, 인터넷에서 제공되는 정보를 통해 문제를 해결하거나 고객 서비스 웹 사이트를 통해 자신의 문제를 진단할 수 있도록 하는 등 다양한 방법으로 리사를 처리해 왔습니다.'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(total_data_ranking2chosen)"
      ],
      "metadata": {
        "id": "85ca93LKB5xN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9780225b-d5bd-4602-eb75-f56064115a09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30660"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare for data and dataset\n",
        "import random\n",
        "random.seed(230319)\n",
        "# list_tmp = list(range(10))\n",
        "random.shuffle(total_data_ranking2chosen)\n",
        "print(total_data_ranking2chosen[45])\n",
        "\n",
        "# train_data = total_data_ranking2chosen[:-1000]  # 29000 학습\n",
        "# eval_data = total_data_ranking2chosen[-1000:0]  # 1000개만 평가\n",
        "\n",
        "train_data = total_data_ranking2chosen[:100]  # 학습 데이터의 수\n",
        "eval_data = total_data_ranking2chosen[100:130]  # 평가 데이터의 수\n",
        "\n",
        "train_dataset = RewardDataset(train_data, tokenizer, args.max_len)\n",
        "eval_dataset = RewardDataset(eval_data, tokenizer, args.max_len)\n",
        "\n",
        "# check\n",
        "idx = 10\n",
        "print('#'*70)\n",
        "print('## prompt ##')\n",
        "print(train_data[idx]['prompt'])\n",
        "print('#'*70)\n",
        "print('## chosen ##')\n",
        "print(train_data[idx]['chosen'])\n",
        "print('#'*70)\n",
        "print('## rejected ##')\n",
        "print(train_data[idx]['rejected'])"
      ],
      "metadata": {
        "id": "kv358luNOX8W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e16c1ae2-0ea6-495b-f092-cccf99146d13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'prompt': '유아인이 류승완 감독을 만나 영화 베테랑의 시나리오를 받았던 곳은?', 'chosen': '유아인이 류승완 감독을 만나 영화 베테랑의 시나리오를 받았던 곳은 류승완의 사무실입니다.', 'rejected': '대구 영화사옥'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:00<00:00, 1345.65it/s]\n",
            "100%|██████████| 30/30 [00:00<00:00, 1467.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "######################################################################\n",
            "## prompt ##\n",
            "체크인 되나요?\n",
            "######################################################################\n",
            "## chosen ##\n",
            "제가 AI 챗봇이기 때문에 호텔이나 항공편 등으로 어떤 체크인을 말씀하시는 것인지 구체적으로 설명해주시면 답변을 드리겠습니다.\n",
            "######################################################################\n",
            "## rejected ##\n",
            "다시 한번 가지게임이지 않아 가지게임이지\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# configure optimizer\n",
        "if args.strategy.startswith('colossalai'):\n",
        "    optim = HybridAdam(model.parameters(), lr=5e-5)\n",
        "else:\n",
        "    optim = Adam(model.parameters(), lr=5e-5)"
      ],
      "metadata": {
        "id": "rq99mcy2OjIT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# batch_size here is expected to be C(k,2), k means # response of each prompt\n",
        "# be limited with the format of dataset 'Dahoas/rm-static', we'd better use batch_size as 1\n",
        "trainer = RewardModelTrainer(model=model,\n",
        "                             strategy=strategy,\n",
        "                             optim=optim,\n",
        "                             train_dataset=train_dataset,\n",
        "                             eval_dataset=eval_dataset,\n",
        "                             batch_size=args.batch_size,\n",
        "                             max_epochs=args.max_epochs)"
      ],
      "metadata": {
        "id": "2Ua2vKRgOkjr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##RM을 통해 각 순위에 대해 점수를 부여하고 가장 정확한 응답에 대해 학습함"
      ],
      "metadata": {
        "id": "m25-9pd7Oq3d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train!!\n",
        "trainer.fit(use_lora=args.lora_rank)\n",
        "\n",
        "## save\n",
        "# save model checkpoint after fitting on only rank0\n",
        "strategy.save_model(model, os.path.join(args.output_dir, 'RM.pt'), only_rank0=True)\n",
        "# save optimizer checkpoint on all ranks\n",
        "strategy.save_optimizer(optim,\n",
        "                        os.path.join(args.output_dir, 'RM_optim_checkpoint_%d.pt' % (torch.cuda.current_device())),\n",
        "                        only_rank0=False)"
      ],
      "metadata": {
        "id": "m5DeYx6jOmWv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e779d852-4dfd-41bf-a459-7ddcc17af896"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train epoch:   0%|          | 0/3 [00:00<?, ?it/s]\n",
            "Train step of epoch 0:   0%|          | 0/25 [00:00<?, ?it/s]\u001b[A\n",
            "Train step of epoch 0:   4%|▍         | 1/25 [00:03<01:31,  3.80s/it]\u001b[A\n",
            "Train step of epoch 0:   4%|▍         | 1/25 [00:03<01:31,  3.80s/it, loss=0.653]\u001b[A\n",
            "Train step of epoch 0:   8%|▊         | 2/25 [00:04<00:46,  2.03s/it, loss=0.653]\u001b[A\n",
            "Train step of epoch 0:   8%|▊         | 2/25 [00:04<00:46,  2.03s/it, loss=0.995]\u001b[A\n",
            "Train step of epoch 0:  12%|█▏        | 3/25 [00:05<00:32,  1.47s/it, loss=0.995]\u001b[A\n",
            "Train step of epoch 0:  12%|█▏        | 3/25 [00:05<00:32,  1.47s/it, loss=0.482]\u001b[A\n",
            "Train step of epoch 0:  16%|█▌        | 4/25 [00:06<00:25,  1.20s/it, loss=0.482]\u001b[A\n",
            "Train step of epoch 0:  16%|█▌        | 4/25 [00:06<00:25,  1.20s/it, loss=0.27] \u001b[A\n",
            "Train step of epoch 0:  20%|██        | 5/25 [00:06<00:21,  1.06s/it, loss=0.27]\u001b[A\n",
            "Train step of epoch 0:  20%|██        | 5/25 [00:07<00:21,  1.06s/it, loss=1]   \u001b[A\n",
            "Train step of epoch 0:  24%|██▍       | 6/25 [00:07<00:18,  1.03it/s, loss=1]\u001b[A\n",
            "Train step of epoch 0:  24%|██▍       | 6/25 [00:07<00:18,  1.03it/s, loss=0.152]\u001b[A\n",
            "Train step of epoch 0:  28%|██▊       | 7/25 [00:08<00:16,  1.09it/s, loss=0.152]\u001b[A\n",
            "Train step of epoch 0:  28%|██▊       | 7/25 [00:08<00:16,  1.09it/s, loss=0.601]\u001b[A\n",
            "Train step of epoch 0:  32%|███▏      | 8/25 [00:09<00:14,  1.14it/s, loss=0.601]\u001b[A\n",
            "Train step of epoch 0:  32%|███▏      | 8/25 [00:09<00:14,  1.14it/s, loss=1.85] \u001b[A\n",
            "Train step of epoch 0:  36%|███▌      | 9/25 [00:10<00:13,  1.17it/s, loss=1.85]\u001b[A\n",
            "Train step of epoch 0:  36%|███▌      | 9/25 [00:10<00:13,  1.17it/s, loss=0.381]\u001b[A\n",
            "Train step of epoch 0:  40%|████      | 10/25 [00:11<00:12,  1.19it/s, loss=0.381]\u001b[A\n",
            "Train step of epoch 0:  40%|████      | 10/25 [00:11<00:12,  1.19it/s, loss=0.829]\u001b[A\n",
            "Train step of epoch 0:  44%|████▍     | 11/25 [00:11<00:11,  1.20it/s, loss=0.829]\u001b[A\n",
            "Train step of epoch 0:  44%|████▍     | 11/25 [00:11<00:11,  1.20it/s, loss=0.478]\u001b[A\n",
            "Train step of epoch 0:  48%|████▊     | 12/25 [00:12<00:10,  1.21it/s, loss=0.478]\u001b[A\n",
            "Train step of epoch 0:  48%|████▊     | 12/25 [00:12<00:10,  1.21it/s, loss=0.469]\u001b[A\n",
            "Train step of epoch 0:  52%|█████▏    | 13/25 [00:13<00:09,  1.22it/s, loss=0.469]\u001b[A\n",
            "Train step of epoch 0:  52%|█████▏    | 13/25 [00:13<00:09,  1.22it/s, loss=0.238]\u001b[A\n",
            "Train step of epoch 0:  56%|█████▌    | 14/25 [00:14<00:08,  1.23it/s, loss=0.238]\u001b[A\n",
            "Train step of epoch 0:  56%|█████▌    | 14/25 [00:14<00:08,  1.23it/s, loss=1.09] \u001b[A\n",
            "Train step of epoch 0:  60%|██████    | 15/25 [00:15<00:08,  1.23it/s, loss=1.09]\u001b[A\n",
            "Train step of epoch 0:  60%|██████    | 15/25 [00:15<00:08,  1.23it/s, loss=0.493]\u001b[A\n",
            "Train step of epoch 0:  64%|██████▍   | 16/25 [00:15<00:07,  1.23it/s, loss=0.493]\u001b[A\n",
            "Train step of epoch 0:  64%|██████▍   | 16/25 [00:15<00:07,  1.23it/s, loss=1.49] \u001b[A\n",
            "Train step of epoch 0:  68%|██████▊   | 17/25 [00:16<00:06,  1.23it/s, loss=1.49]\u001b[A\n",
            "Train step of epoch 0:  68%|██████▊   | 17/25 [00:16<00:06,  1.23it/s, loss=0.995]\u001b[A\n",
            "Train step of epoch 0:  72%|███████▏  | 18/25 [00:17<00:05,  1.23it/s, loss=0.995]\u001b[A\n",
            "Train step of epoch 0:  72%|███████▏  | 18/25 [00:17<00:05,  1.23it/s, loss=0.405]\u001b[A\n",
            "Train step of epoch 0:  76%|███████▌  | 19/25 [00:18<00:04,  1.24it/s, loss=0.405]\u001b[A\n",
            "Train step of epoch 0:  76%|███████▌  | 19/25 [00:18<00:04,  1.24it/s, loss=0.824]\u001b[A\n",
            "Train step of epoch 0:  80%|████████  | 20/25 [00:19<00:04,  1.24it/s, loss=0.824]\u001b[A\n",
            "Train step of epoch 0:  80%|████████  | 20/25 [00:19<00:04,  1.24it/s, loss=0.502]\u001b[A\n",
            "Train step of epoch 0:  84%|████████▍ | 21/25 [00:19<00:03,  1.24it/s, loss=0.502]\u001b[A\n",
            "Train step of epoch 0:  84%|████████▍ | 21/25 [00:19<00:03,  1.24it/s, loss=0.671]\u001b[A\n",
            "Train step of epoch 0:  88%|████████▊ | 22/25 [00:20<00:02,  1.24it/s, loss=0.671]\u001b[A\n",
            "Train step of epoch 0:  88%|████████▊ | 22/25 [00:20<00:02,  1.24it/s, loss=0.673]\u001b[A\n",
            "Train step of epoch 0:  92%|█████████▏| 23/25 [00:21<00:01,  1.23it/s, loss=0.673]\u001b[A\n",
            "Train step of epoch 0:  92%|█████████▏| 23/25 [00:21<00:01,  1.23it/s, loss=0.858]\u001b[A\n",
            "Train step of epoch 0:  96%|█████████▌| 24/25 [00:22<00:00,  1.23it/s, loss=0.858]\u001b[A\n",
            "Train step of epoch 0:  96%|█████████▌| 24/25 [00:22<00:00,  1.23it/s, loss=0.561]\u001b[A\n",
            "Train step of epoch 0: 100%|██████████| 25/25 [00:23<00:00,  1.23it/s, loss=0.561]\u001b[A\n",
            "Train epoch:  33%|███▎      | 1/3 [00:25<00:50, 25.15s/it]\n",
            "Train step of epoch 0: 100%|██████████| 25/25 [00:25<00:00,  1.01s/it, loss=0.583, dist_mean=0.299]\n",
            "\n",
            "Train step of epoch 1:   0%|          | 0/25 [00:00<?, ?it/s]\u001b[A\n",
            "Train step of epoch 1:   4%|▍         | 1/25 [00:00<00:18,  1.29it/s]\u001b[A\n",
            "Train step of epoch 1:   4%|▍         | 1/25 [00:00<00:18,  1.29it/s, loss=0.526]\u001b[A\n",
            "Train step of epoch 1:   8%|▊         | 2/25 [00:01<00:18,  1.26it/s, loss=0.526]\u001b[A\n",
            "Train step of epoch 1:   8%|▊         | 2/25 [00:01<00:18,  1.26it/s, loss=0.944]\u001b[A\n",
            "Train step of epoch 1:  12%|█▏        | 3/25 [00:02<00:17,  1.24it/s, loss=0.944]\u001b[A\n",
            "Train step of epoch 1:  12%|█▏        | 3/25 [00:02<00:17,  1.24it/s, loss=0.597]\u001b[A\n",
            "Train step of epoch 1:  16%|█▌        | 4/25 [00:03<00:17,  1.23it/s, loss=0.597]\u001b[A\n",
            "Train step of epoch 1:  16%|█▌        | 4/25 [00:03<00:17,  1.23it/s, loss=0.571]\u001b[A\n",
            "Train step of epoch 1:  20%|██        | 5/25 [00:04<00:16,  1.23it/s, loss=0.571]\u001b[A\n",
            "Train step of epoch 1:  20%|██        | 5/25 [00:04<00:16,  1.23it/s, loss=0.674]\u001b[A\n",
            "Train step of epoch 1:  24%|██▍       | 6/25 [00:04<00:15,  1.23it/s, loss=0.674]\u001b[A\n",
            "Train step of epoch 1:  24%|██▍       | 6/25 [00:04<00:15,  1.23it/s, loss=0.826]\u001b[A\n",
            "Train step of epoch 1:  28%|██▊       | 7/25 [00:05<00:14,  1.23it/s, loss=0.826]\u001b[A\n",
            "Train step of epoch 1:  28%|██▊       | 7/25 [00:05<00:14,  1.23it/s, loss=0.708]\u001b[A\n",
            "Train step of epoch 1:  32%|███▏      | 8/25 [00:06<00:13,  1.23it/s, loss=0.708]\u001b[A\n",
            "Train step of epoch 1:  32%|███▏      | 8/25 [00:06<00:13,  1.23it/s, loss=0.595]\u001b[A\n",
            "Train step of epoch 1:  36%|███▌      | 9/25 [00:07<00:13,  1.23it/s, loss=0.595]\u001b[A\n",
            "Train step of epoch 1:  36%|███▌      | 9/25 [00:07<00:13,  1.23it/s, loss=0.517]\u001b[A\n",
            "Train step of epoch 1:  40%|████      | 10/25 [00:08<00:12,  1.22it/s, loss=0.517]\u001b[A\n",
            "Train step of epoch 1:  40%|████      | 10/25 [00:08<00:12,  1.22it/s, loss=0.507]\u001b[A\n",
            "Train step of epoch 1:  44%|████▍     | 11/25 [00:08<00:11,  1.23it/s, loss=0.507]\u001b[A\n",
            "Train step of epoch 1:  44%|████▍     | 11/25 [00:08<00:11,  1.23it/s, loss=0.391]\u001b[A\n",
            "Train step of epoch 1:  48%|████▊     | 12/25 [00:09<00:10,  1.23it/s, loss=0.391]\u001b[A\n",
            "Train step of epoch 1:  48%|████▊     | 12/25 [00:09<00:10,  1.23it/s, loss=0.347]\u001b[A\n",
            "Train step of epoch 1:  52%|█████▏    | 13/25 [00:10<00:09,  1.23it/s, loss=0.347]\u001b[A\n",
            "Train step of epoch 1:  52%|█████▏    | 13/25 [00:10<00:09,  1.23it/s, loss=0.207]\u001b[A\n",
            "Train step of epoch 1:  56%|█████▌    | 14/25 [00:11<00:08,  1.23it/s, loss=0.207]\u001b[A\n",
            "Train step of epoch 1:  56%|█████▌    | 14/25 [00:11<00:08,  1.23it/s, loss=0.556]\u001b[A\n",
            "Train step of epoch 1:  60%|██████    | 15/25 [00:12<00:08,  1.22it/s, loss=0.556]\u001b[A\n",
            "Train step of epoch 1:  60%|██████    | 15/25 [00:12<00:08,  1.22it/s, loss=0.305]\u001b[A\n",
            "Train step of epoch 1:  64%|██████▍   | 16/25 [00:13<00:07,  1.22it/s, loss=0.305]\u001b[A\n",
            "Train step of epoch 1:  64%|██████▍   | 16/25 [00:13<00:07,  1.22it/s, loss=0.714]\u001b[A\n",
            "Train step of epoch 1:  68%|██████▊   | 17/25 [00:13<00:06,  1.22it/s, loss=0.714]\u001b[A\n",
            "Train step of epoch 1:  68%|██████▊   | 17/25 [00:13<00:06,  1.22it/s, loss=0.737]\u001b[A\n",
            "Train step of epoch 1:  72%|███████▏  | 18/25 [00:14<00:05,  1.23it/s, loss=0.737]\u001b[A\n",
            "Train step of epoch 1:  72%|███████▏  | 18/25 [00:14<00:05,  1.23it/s, loss=0.293]\u001b[A\n",
            "Train step of epoch 1:  76%|███████▌  | 19/25 [00:15<00:04,  1.22it/s, loss=0.293]\u001b[A\n",
            "Train step of epoch 1:  76%|███████▌  | 19/25 [00:15<00:04,  1.22it/s, loss=0.67] \u001b[A\n",
            "Train step of epoch 1:  80%|████████  | 20/25 [00:16<00:04,  1.22it/s, loss=0.67]\u001b[A\n",
            "Train step of epoch 1:  80%|████████  | 20/25 [00:16<00:04,  1.22it/s, loss=0.266]\u001b[A\n",
            "Train step of epoch 1:  84%|████████▍ | 21/25 [00:17<00:03,  1.22it/s, loss=0.266]\u001b[A\n",
            "Train step of epoch 1:  84%|████████▍ | 21/25 [00:17<00:03,  1.22it/s, loss=0.675]\u001b[A\n",
            "Train step of epoch 1:  88%|████████▊ | 22/25 [00:17<00:02,  1.22it/s, loss=0.675]\u001b[A\n",
            "Train step of epoch 1:  88%|████████▊ | 22/25 [00:17<00:02,  1.22it/s, loss=1.09] \u001b[A\n",
            "Train step of epoch 1:  92%|█████████▏| 23/25 [00:18<00:01,  1.22it/s, loss=1.09]\u001b[A\n",
            "Train step of epoch 1:  92%|█████████▏| 23/25 [00:18<00:01,  1.22it/s, loss=0.513]\u001b[A\n",
            "Train step of epoch 1:  96%|█████████▌| 24/25 [00:19<00:00,  1.22it/s, loss=0.513]\u001b[A\n",
            "Train step of epoch 1:  96%|█████████▌| 24/25 [00:19<00:00,  1.22it/s, loss=0.602]\u001b[A\n",
            "Train step of epoch 1: 100%|██████████| 25/25 [00:20<00:00,  1.22it/s, loss=0.602]\u001b[A\n",
            "Train epoch:  67%|██████▋   | 2/3 [00:47<00:23, 23.55s/it]\n",
            "Train step of epoch 1: 100%|██████████| 25/25 [00:22<00:00,  1.12it/s, loss=0.483, dist_mean=0.804]\n",
            "\n",
            "Train step of epoch 2:   0%|          | 0/25 [00:00<?, ?it/s]\u001b[A\n",
            "Train step of epoch 2:   4%|▍         | 1/25 [00:00<00:18,  1.27it/s]\u001b[A\n",
            "Train step of epoch 2:   4%|▍         | 1/25 [00:00<00:18,  1.27it/s, loss=0.314]\u001b[A\n",
            "Train step of epoch 2:   8%|▊         | 2/25 [00:01<00:18,  1.24it/s, loss=0.314]\u001b[A\n",
            "Train step of epoch 2:   8%|▊         | 2/25 [00:01<00:18,  1.24it/s, loss=0.305]\u001b[A\n",
            "Train step of epoch 2:  12%|█▏        | 3/25 [00:02<00:17,  1.22it/s, loss=0.305]\u001b[A\n",
            "Train step of epoch 2:  12%|█▏        | 3/25 [00:02<00:17,  1.22it/s, loss=0.304]\u001b[A\n",
            "Train step of epoch 2:  16%|█▌        | 4/25 [00:03<00:17,  1.22it/s, loss=0.304]\u001b[A\n",
            "Train step of epoch 2:  16%|█▌        | 4/25 [00:03<00:17,  1.22it/s, loss=0.436]\u001b[A\n",
            "Train step of epoch 2:  20%|██        | 5/25 [00:04<00:16,  1.22it/s, loss=0.436]\u001b[A\n",
            "Train step of epoch 2:  20%|██        | 5/25 [00:04<00:16,  1.22it/s, loss=0.233]\u001b[A\n",
            "Train step of epoch 2:  24%|██▍       | 6/25 [00:04<00:15,  1.22it/s, loss=0.233]\u001b[A\n",
            "Train step of epoch 2:  24%|██▍       | 6/25 [00:04<00:15,  1.22it/s, loss=0.387]\u001b[A\n",
            "Train step of epoch 2:  28%|██▊       | 7/25 [00:05<00:14,  1.22it/s, loss=0.387]\u001b[A\n",
            "Train step of epoch 2:  28%|██▊       | 7/25 [00:05<00:14,  1.22it/s, loss=0.381]\u001b[A\n",
            "Train step of epoch 2:  32%|███▏      | 8/25 [00:06<00:13,  1.22it/s, loss=0.381]\u001b[A\n",
            "Train step of epoch 2:  32%|███▏      | 8/25 [00:06<00:13,  1.22it/s, loss=0.275]\u001b[A\n",
            "Train step of epoch 2:  36%|███▌      | 9/25 [00:07<00:13,  1.21it/s, loss=0.275]\u001b[A\n",
            "Train step of epoch 2:  36%|███▌      | 9/25 [00:07<00:13,  1.21it/s, loss=0.275]\u001b[A\n",
            "Train step of epoch 2:  40%|████      | 10/25 [00:08<00:12,  1.21it/s, loss=0.275]\u001b[A\n",
            "Train step of epoch 2:  40%|████      | 10/25 [00:08<00:12,  1.21it/s, loss=0.392]\u001b[A\n",
            "Train step of epoch 2:  44%|████▍     | 11/25 [00:09<00:11,  1.21it/s, loss=0.392]\u001b[A\n",
            "Train step of epoch 2:  44%|████▍     | 11/25 [00:09<00:11,  1.21it/s, loss=0.203]\u001b[A\n",
            "Train step of epoch 2:  48%|████▊     | 12/25 [00:09<00:10,  1.22it/s, loss=0.203]\u001b[A\n",
            "Train step of epoch 2:  48%|████▊     | 12/25 [00:09<00:10,  1.22it/s, loss=0.277]\u001b[A\n",
            "Train step of epoch 2:  52%|█████▏    | 13/25 [00:10<00:09,  1.22it/s, loss=0.277]\u001b[A\n",
            "Train step of epoch 2:  52%|█████▏    | 13/25 [00:10<00:09,  1.22it/s, loss=0.0378]\u001b[A\n",
            "Train step of epoch 2:  56%|█████▌    | 14/25 [00:11<00:09,  1.22it/s, loss=0.0378]\u001b[A\n",
            "Train step of epoch 2:  56%|█████▌    | 14/25 [00:11<00:09,  1.22it/s, loss=0.134] \u001b[A\n",
            "Train step of epoch 2:  60%|██████    | 15/25 [00:12<00:08,  1.21it/s, loss=0.134]\u001b[A\n",
            "Train step of epoch 2:  60%|██████    | 15/25 [00:12<00:08,  1.21it/s, loss=0.0736]\u001b[A\n",
            "Train step of epoch 2:  64%|██████▍   | 16/25 [00:13<00:07,  1.21it/s, loss=0.0736]\u001b[A\n",
            "Train step of epoch 2:  64%|██████▍   | 16/25 [00:13<00:07,  1.21it/s, loss=0.355] \u001b[A\n",
            "Train step of epoch 2:  68%|██████▊   | 17/25 [00:13<00:06,  1.21it/s, loss=0.355]\u001b[A\n",
            "Train step of epoch 2:  68%|██████▊   | 17/25 [00:13<00:06,  1.21it/s, loss=0.0232]\u001b[A\n",
            "Train step of epoch 2:  72%|███████▏  | 18/25 [00:14<00:05,  1.21it/s, loss=0.0232]\u001b[A\n",
            "Train step of epoch 2:  72%|███████▏  | 18/25 [00:14<00:05,  1.21it/s, loss=0.00893]\u001b[A\n",
            "Train step of epoch 2:  76%|███████▌  | 19/25 [00:15<00:04,  1.21it/s, loss=0.00893]\u001b[A\n",
            "Train step of epoch 2:  76%|███████▌  | 19/25 [00:15<00:04,  1.21it/s, loss=0.159]  \u001b[A\n",
            "Train step of epoch 2:  80%|████████  | 20/25 [00:16<00:04,  1.21it/s, loss=0.159]\u001b[A\n",
            "Train step of epoch 2:  80%|████████  | 20/25 [00:16<00:04,  1.21it/s, loss=0.00972]\u001b[A\n",
            "Train step of epoch 2:  84%|████████▍ | 21/25 [00:17<00:03,  1.21it/s, loss=0.00972]\u001b[A\n",
            "Train step of epoch 2:  84%|████████▍ | 21/25 [00:17<00:03,  1.21it/s, loss=1.3]    \u001b[A\n",
            "Train step of epoch 2:  88%|████████▊ | 22/25 [00:18<00:02,  1.21it/s, loss=1.3]\u001b[A\n",
            "Train step of epoch 2:  88%|████████▊ | 22/25 [00:18<00:02,  1.21it/s, loss=0.0462]\u001b[A\n",
            "Train step of epoch 2:  92%|█████████▏| 23/25 [00:18<00:01,  1.21it/s, loss=0.0462]\u001b[A\n",
            "Train step of epoch 2:  92%|█████████▏| 23/25 [00:18<00:01,  1.21it/s, loss=0.0791]\u001b[A\n",
            "Train step of epoch 2:  96%|█████████▌| 24/25 [00:19<00:00,  1.21it/s, loss=0.0791]\u001b[A\n",
            "Train step of epoch 2:  96%|█████████▌| 24/25 [00:19<00:00,  1.21it/s, loss=0.0202]\u001b[A\n",
            "Train step of epoch 2: 100%|██████████| 25/25 [00:20<00:00,  1.21it/s, loss=0.0202]\u001b[A\n",
            "Train epoch: 100%|██████████| 3/3 [01:10<00:00, 23.14s/it]\n",
            "Train step of epoch 2: 100%|██████████| 25/25 [00:22<00:00,  1.10it/s, loss=0.861, dist_mean=1.35]\n",
            "Train epoch: 100%|██████████| 3/3 [01:10<00:00, 23.42s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3) PPO 사람의 피드백을 반영하여 질문에 대한 응답이 피드백과 비슷해 지도록 학습"
      ],
      "metadata": {
        "id": "-vFmdP46P3cz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## setup(1min)\n",
        "# for ColossalAI\n",
        "!pip install colossalai==0.2.7\n",
        "!pip install torch==1.13.1\n",
        "# setup data\n",
        "\n",
        "%cd /content/drive/MyDrive/AI/colossalai/colossalai_ChatGPT/\n",
        "!pip install .\n",
        "%cd ../../\n",
        "\n",
        "# setup library\n",
        "!pip install openai\n",
        "!pip install langchain==0.0.113\n",
        "!pip install pandas>=1.4.1"
      ],
      "metadata": {
        "id": "l_vRtPXodlmf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "628854aa-61fd-4714-9106-c68080a917fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: colossalai==0.2.7 in /usr/local/lib/python3.10/dist-packages (0.2.7)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from colossalai==0.2.7) (1.22.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from colossalai==0.2.7) (4.65.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from colossalai==0.2.7) (5.9.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from colossalai==0.2.7) (23.1)\n",
            "Requirement already satisfied: pre-commit in /usr/local/lib/python3.10/dist-packages (from colossalai==0.2.7) (3.3.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from colossalai==0.2.7) (13.3.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from colossalai==0.2.7) (8.1.3)\n",
            "Requirement already satisfied: fabric in /usr/local/lib/python3.10/dist-packages (from colossalai==0.2.7) (3.0.1)\n",
            "Requirement already satisfied: contexttimer in /usr/local/lib/python3.10/dist-packages (from colossalai==0.2.7) (0.3.3)\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.10/dist-packages (from colossalai==0.2.7) (1.11.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from colossalai==0.2.7) (1.13.1)\n",
            "Requirement already satisfied: invoke>=2.0 in /usr/local/lib/python3.10/dist-packages (from fabric->colossalai==0.2.7) (2.1.2)\n",
            "Requirement already satisfied: paramiko>=2.4 in /usr/local/lib/python3.10/dist-packages (from fabric->colossalai==0.2.7) (3.1.0)\n",
            "Requirement already satisfied: cfgv>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pre-commit->colossalai==0.2.7) (3.3.1)\n",
            "Requirement already satisfied: identify>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pre-commit->colossalai==0.2.7) (2.5.24)\n",
            "Requirement already satisfied: nodeenv>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from pre-commit->colossalai==0.2.7) (1.8.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from pre-commit->colossalai==0.2.7) (6.0)\n",
            "Requirement already satisfied: virtualenv>=20.10.0 in /usr/local/lib/python3.10/dist-packages (from pre-commit->colossalai==0.2.7) (20.23.0)\n",
            "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->colossalai==0.2.7) (2.2.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->colossalai==0.2.7) (2.14.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->colossalai==0.2.7) (4.5.0)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch->colossalai==0.2.7) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.10/dist-packages (from torch->colossalai==0.2.7) (8.5.0.96)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.10/dist-packages (from torch->colossalai==0.2.7) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch->colossalai==0.2.7) (11.7.99)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch->colossalai==0.2.7) (67.7.2)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch->colossalai==0.2.7) (0.40.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->colossalai==0.2.7) (0.1.2)\n",
            "Requirement already satisfied: bcrypt>=3.2 in /usr/local/lib/python3.10/dist-packages (from paramiko>=2.4->fabric->colossalai==0.2.7) (4.0.1)\n",
            "Requirement already satisfied: cryptography>=3.3 in /usr/local/lib/python3.10/dist-packages (from paramiko>=2.4->fabric->colossalai==0.2.7) (40.0.2)\n",
            "Requirement already satisfied: pynacl>=1.5 in /usr/local/lib/python3.10/dist-packages (from paramiko>=2.4->fabric->colossalai==0.2.7) (1.5.0)\n",
            "Requirement already satisfied: distlib<1,>=0.3.6 in /usr/local/lib/python3.10/dist-packages (from virtualenv>=20.10.0->pre-commit->colossalai==0.2.7) (0.3.6)\n",
            "Requirement already satisfied: filelock<4,>=3.11 in /usr/local/lib/python3.10/dist-packages (from virtualenv>=20.10.0->pre-commit->colossalai==0.2.7) (3.12.0)\n",
            "Requirement already satisfied: platformdirs<4,>=3.2 in /usr/local/lib/python3.10/dist-packages (from virtualenv>=20.10.0->pre-commit->colossalai==0.2.7) (3.3.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=3.3->paramiko>=2.4->fabric->colossalai==0.2.7) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=3.3->paramiko>=2.4->fabric->colossalai==0.2.7) (2.21)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch==1.13.1 in /usr/local/lib/python3.10/dist-packages (1.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==1.13.1) (4.5.0)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch==1.13.1) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.10/dist-packages (from torch==1.13.1) (8.5.0.96)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.10/dist-packages (from torch==1.13.1) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch==1.13.1) (11.7.99)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.1) (67.7.2)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.1) (0.40.0)\n",
            "/content/drive/MyDrive/AI/colossalai/colossalai_ChatGPT\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Processing /content/drive/MyDrive/AI/colossalai/colossalai_ChatGPT\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: transformers>=4.20.1 in /usr/local/lib/python3.10/dist-packages (from chatgpt==0.1.0) (4.28.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from chatgpt==0.1.0) (4.65.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (from chatgpt==0.1.0) (2.12.0)\n",
            "Requirement already satisfied: loralib in /usr/local/lib/python3.10/dist-packages (from chatgpt==0.1.0) (0.1.1)\n",
            "Requirement already satisfied: colossalai>=0.2.4 in /usr/local/lib/python3.10/dist-packages (from chatgpt==0.1.0) (0.2.7)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from chatgpt==0.1.0) (1.13.1)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (from chatgpt==0.1.0) (0.0.113)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.2.4->chatgpt==0.1.0) (1.22.4)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.2.4->chatgpt==0.1.0) (5.9.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.2.4->chatgpt==0.1.0) (23.1)\n",
            "Requirement already satisfied: pre-commit in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.2.4->chatgpt==0.1.0) (3.3.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.2.4->chatgpt==0.1.0) (13.3.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.2.4->chatgpt==0.1.0) (8.1.3)\n",
            "Requirement already satisfied: fabric in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.2.4->chatgpt==0.1.0) (3.0.1)\n",
            "Requirement already satisfied: contexttimer in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.2.4->chatgpt==0.1.0) (0.3.3)\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.2.4->chatgpt==0.1.0) (1.11.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers>=4.20.1->chatgpt==0.1.0) (3.12.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.20.1->chatgpt==0.1.0) (0.14.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.20.1->chatgpt==0.1.0) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.20.1->chatgpt==0.1.0) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers>=4.20.1->chatgpt==0.1.0) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.20.1->chatgpt==0.1.0) (0.13.3)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->chatgpt==0.1.0) (9.0.0)\n",
            "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets->chatgpt==0.1.0) (0.3.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->chatgpt==0.1.0) (1.5.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->chatgpt==0.1.0) (3.2.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets->chatgpt==0.1.0) (0.70.14)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets->chatgpt==0.1.0) (2023.4.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->chatgpt==0.1.0) (3.8.4)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.10/dist-packages (from datasets->chatgpt==0.1.0) (0.18.0)\n",
            "Requirement already satisfied: SQLAlchemy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain->chatgpt==0.1.0) (1.4.48)\n",
            "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain->chatgpt==0.1.0) (0.5.7)\n",
            "Requirement already satisfied: pydantic<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain->chatgpt==0.1.0) (1.10.7)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain->chatgpt==0.1.0) (8.2.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->chatgpt==0.1.0) (4.5.0)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch->chatgpt==0.1.0) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.10/dist-packages (from torch->chatgpt==0.1.0) (8.5.0.96)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.10/dist-packages (from torch->chatgpt==0.1.0) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch->chatgpt==0.1.0) (11.7.99)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch->chatgpt==0.1.0) (67.7.2)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch->chatgpt==0.1.0) (0.40.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->chatgpt==0.1.0) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->chatgpt==0.1.0) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->chatgpt==0.1.0) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->chatgpt==0.1.0) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->chatgpt==0.1.0) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->chatgpt==0.1.0) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->chatgpt==0.1.0) (1.3.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain->chatgpt==0.1.0) (3.19.0)\n",
            "Requirement already satisfied: marshmallow-enum<2.0.0,>=1.5.1 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain->chatgpt==0.1.0) (1.5.1)\n",
            "Requirement already satisfied: typing-inspect>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain->chatgpt==0.1.0) (0.8.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.20.1->chatgpt==0.1.0) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.20.1->chatgpt==0.1.0) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.20.1->chatgpt==0.1.0) (3.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<2,>=1->langchain->chatgpt==0.1.0) (2.0.2)\n",
            "Requirement already satisfied: invoke>=2.0 in /usr/local/lib/python3.10/dist-packages (from fabric->colossalai>=0.2.4->chatgpt==0.1.0) (2.1.2)\n",
            "Requirement already satisfied: paramiko>=2.4 in /usr/local/lib/python3.10/dist-packages (from fabric->colossalai>=0.2.4->chatgpt==0.1.0) (3.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->chatgpt==0.1.0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->chatgpt==0.1.0) (2022.7.1)\n",
            "Requirement already satisfied: cfgv>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pre-commit->colossalai>=0.2.4->chatgpt==0.1.0) (3.3.1)\n",
            "Requirement already satisfied: identify>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pre-commit->colossalai>=0.2.4->chatgpt==0.1.0) (2.5.24)\n",
            "Requirement already satisfied: nodeenv>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from pre-commit->colossalai>=0.2.4->chatgpt==0.1.0) (1.8.0)\n",
            "Requirement already satisfied: virtualenv>=20.10.0 in /usr/local/lib/python3.10/dist-packages (from pre-commit->colossalai>=0.2.4->chatgpt==0.1.0) (20.23.0)\n",
            "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->colossalai>=0.2.4->chatgpt==0.1.0) (2.2.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->colossalai>=0.2.4->chatgpt==0.1.0) (2.14.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->colossalai>=0.2.4->chatgpt==0.1.0) (0.1.2)\n",
            "Requirement already satisfied: bcrypt>=3.2 in /usr/local/lib/python3.10/dist-packages (from paramiko>=2.4->fabric->colossalai>=0.2.4->chatgpt==0.1.0) (4.0.1)\n",
            "Requirement already satisfied: cryptography>=3.3 in /usr/local/lib/python3.10/dist-packages (from paramiko>=2.4->fabric->colossalai>=0.2.4->chatgpt==0.1.0) (40.0.2)\n",
            "Requirement already satisfied: pynacl>=1.5 in /usr/local/lib/python3.10/dist-packages (from paramiko>=2.4->fabric->colossalai>=0.2.4->chatgpt==0.1.0) (1.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets->chatgpt==0.1.0) (1.16.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain->chatgpt==0.1.0) (1.0.0)\n",
            "Requirement already satisfied: distlib<1,>=0.3.6 in /usr/local/lib/python3.10/dist-packages (from virtualenv>=20.10.0->pre-commit->colossalai>=0.2.4->chatgpt==0.1.0) (0.3.6)\n",
            "Requirement already satisfied: platformdirs<4,>=3.2 in /usr/local/lib/python3.10/dist-packages (from virtualenv>=20.10.0->pre-commit->colossalai>=0.2.4->chatgpt==0.1.0) (3.3.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=3.3->paramiko>=2.4->fabric->colossalai>=0.2.4->chatgpt==0.1.0) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=3.3->paramiko>=2.4->fabric->colossalai>=0.2.4->chatgpt==0.1.0) (2.21)\n",
            "Building wheels for collected packages: chatgpt\n",
            "  Building wheel for chatgpt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for chatgpt: filename=chatgpt-0.1.0-py3-none-any.whl size=46647 sha256=d41b77f4cd255e47c6887966ac1f48ec0b21103593f500597f18221aaf4d4407\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-ru6a2o2t/wheels/68/75/db/a19a8451de3c93bfef08e23d928e547a812f71776c4b8ef5ef\n",
            "Successfully built chatgpt\n",
            "Installing collected packages: chatgpt\n",
            "  Attempting uninstall: chatgpt\n",
            "    Found existing installation: chatgpt 0.1.0\n",
            "    Uninstalling chatgpt-0.1.0:\n",
            "      Successfully uninstalled chatgpt-0.1.0\n",
            "Successfully installed chatgpt-0.1.0\n",
            "/content/drive/MyDrive/AI\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (0.27.7)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.65.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.8.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: langchain==0.0.113 in /usr/local/lib/python3.10/dist-packages (0.0.113)\n",
            "Requirement already satisfied: PyYAML<7,>=6 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.113) (6.0)\n",
            "Requirement already satisfied: SQLAlchemy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.113) (1.4.48)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.113) (3.8.4)\n",
            "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.113) (0.5.7)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.113) (1.22.4)\n",
            "Requirement already satisfied: pydantic<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.113) (1.10.7)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.113) (2.27.1)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.113) (8.2.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.113) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.113) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.113) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.113) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.113) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.113) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.113) (1.3.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.113) (3.19.0)\n",
            "Requirement already satisfied: marshmallow-enum<2.0.0,>=1.5.1 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.113) (1.5.1)\n",
            "Requirement already satisfied: typing-inspect>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.113) (0.8.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2,>=1->langchain==0.0.113) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.113) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.113) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.113) (3.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<2,>=1->langchain==0.0.113) (2.0.2)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.113) (23.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.113) (1.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import\n",
        "import argparse\n",
        "from copy import deepcopy\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "from chatgpt.models.base import RewardModel\n",
        "from chatgpt.models.bloom import BLOOMActor, BLOOMCritic\n",
        "from chatgpt.models.gpt import GPTActor, GPTCritic\n",
        "from chatgpt.models.opt import OPTActor, OPTCritic\n",
        "from chatgpt.trainer import PPOTrainer\n",
        "from chatgpt.trainer.strategies import ColossalAIStrategy, DDPStrategy, NaiveStrategy\n",
        "from torch.optim import Adam\n",
        "from transformers import AutoTokenizer, BloomTokenizerFast\n",
        "from transformers.models.gpt2.tokenization_gpt2 import GPT2Tokenizer\n",
        "\n",
        "from colossalai.nn.optimizer import HybridAdam\n",
        "\n",
        "## wy 추가\n",
        "import json\n",
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "\n",
        "## clossalAI error 해결\n",
        "os.environ['RANK'] = '0'\n",
        "os.environ['LOCAL_RANK'] = '0'\n",
        "os.environ['WORLD_SIZE'] = '2'\n",
        "os.environ['MASTER_ADDR'] = '127.0.0.1'\n",
        "os.environ['MASTER_PORT'] = '42043'\n",
        "\n",
        "# data config\n",
        "IGNORE_INDEX = -100\n",
        "DEFAULT_PAD_TOKEN = \"[PAD]\"\n",
        "DEFAULT_EOS_TOKEN = \"</s>\"\n",
        "DEFAULT_BOS_TOKEN = \"</s>\"\n",
        "DEFAULT_UNK_TOKEN = \"</s>\"\n",
        "PROMPT_DICT = {\n",
        "    \"prompt_input\": (\n",
        "        \"Instruction(명령어):{prompt}\\n\\n### Input(입력):\\n{input}\\n\\nResponse(응답):\"\n",
        "    ),\n",
        "    \"prompt_no_input\": (\n",
        "        \"Instruction(명령어):{prompt}\\n\\nResponse(응답):\"\n",
        "    ),\n",
        "}"
      ],
      "metadata": {
        "id": "RTa3nVDuO5Nk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8468888-dcff-4b58-9df7-c00fb959feee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/library.py:130: UserWarning: Overriding a previously registered kernel for the same operator and the same dispatch key\n",
            "  operator: aten::index.Tensor(Tensor self, Tensor?[] indices) -> Tensor\n",
            "    registered at aten/src/ATen/RegisterSchema.cpp:6\n",
            "  dispatch key: Meta\n",
            "  previous kernel: registered at ../aten/src/ATen/functorch/BatchRulesScatterOps.cpp:1053\n",
            "       new kernel: registered at /dev/null:241 (Triggered internally at ../aten/src/ATen/core/dispatch/OperatorEntry.cpp:150.)\n",
            "  self.m.impl(name, dispatch_key, fn)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# define argment\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--data_path_3_PPO', type=str, default='/content/drive/MyDrive/AI/Kochatgpt_data/kochatgpt_3_PPO.jsonl')\n",
        "parser.add_argument('--output_dir', type=str, default='./output_3_PPO')\n",
        "parser.add_argument('--strategy',\n",
        "                    choices=['naive', 'ddp', 'colossalai_gemini', 'colossalai_zero2'],\n",
        "                    default='naive')\n",
        "parser.add_argument('--model', type=str, default='gpt2', choices=['gpt2', 'bloom', 'opt'])\n",
        "parser.add_argument('--pretrain', type=str, default=None)\n",
        "parser.add_argument('--num_episodes', type=int, default=10)\n",
        "parser.add_argument('--max_timesteps', type=int, default=3)\n",
        "parser.add_argument('--update_timesteps', type=int, default=3)\n",
        "parser.add_argument('--max_epochs', type=int, default=5)\n",
        "parser.add_argument('--train_batch_size', type=int, default=8)\n",
        "parser.add_argument('--lora_rank', type=int, default=0, help=\"low-rank adaptation matrices rank\")\n",
        "parser.add_argument('--max_length', type=int, default=250)\n",
        "args = parser.parse_args(args=[])\n",
        "\n",
        "# for test\n",
        "args.output_dir = './output_3_PPO'\n",
        "args.pretrain = 'skt/kogpt2-base-v2'  # pretrained 모델 가져오기\n",
        "\n",
        "# args.pretrain_actor = './output_1_SFT'  # SFT 모델 가져오기\n",
        "# args.pretrain_critic = './output_2_RM'  # RM 모델 가져오기\n",
        "args.pretrain_actor = args.pretrain\n",
        "args.pretrain_critic = args.pretrain\n",
        "\n",
        "args.num_episodes = 1\n",
        "args.max_epochs   = 1\n",
        "\n",
        "print(args)\n",
        "if not os.path.exists(args.output_dir):\n",
        "    os.makedirs(args.output_dir)"
      ],
      "metadata": {
        "id": "LzLnJEHZPB2T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "857422e4-a0e5-491b-c7e7-50cd90021bfe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(data_path_3_PPO='/content/drive/MyDrive/AI/Kochatgpt_data/kochatgpt_3_PPO.jsonl', output_dir='./output_3_PPO', strategy='naive', model='gpt2', pretrain='skt/kogpt2-base-v2', num_episodes=1, max_timesteps=3, update_timesteps=3, max_epochs=1, train_batch_size=8, lora_rank=0, max_length=250, pretrain_actor='skt/kogpt2-base-v2', pretrain_critic='skt/kogpt2-base-v2')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "args.pretrain"
      ],
      "metadata": {
        "id": "Pisr96oIccRT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# configure strategy\n",
        "if args.strategy == 'naive':\n",
        "    strategy = NaiveStrategy()\n",
        "elif args.strategy == 'ddp':\n",
        "    strategy = DDPStrategy()\n",
        "elif args.strategy == 'colossalai_gemini':\n",
        "    strategy = ColossalAIStrategy(stage=3, placement_policy='cuda')\n",
        "elif args.strategy == 'colossalai_zero2':\n",
        "    strategy = ColossalAIStrategy(stage=2, placement_policy='cuda')\n",
        "else:\n",
        "    raise ValueError(f'Unsupported strategy \"{args.strategy}\"')"
      ],
      "metadata": {
        "id": "a9xRGXwiPNs3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# configure model, tokenizer\n",
        "with strategy.model_init_context():\n",
        "    if args.model == 'gpt2':\n",
        "        actor = GPTActor(pretrained=args.pretrain_actor, lora_rank=args.lora_rank).to(torch.cuda.current_device())\n",
        "        critic = GPTCritic(pretrained=args.pretrain_critic, lora_rank=args.lora_rank).to(torch.cuda.current_device())\n",
        "        # tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "        # tokenizer.pad_token = tokenizer.eos_token\n",
        "        tokenizer = AutoTokenizer.from_pretrained(args.pretrain, padding_side=\"right\", model_max_length=512)\n",
        "        tokenizer.add_special_tokens(\n",
        "            {\n",
        "                \"eos_token\": DEFAULT_EOS_TOKEN,\n",
        "                \"bos_token\": DEFAULT_BOS_TOKEN,\n",
        "                \"unk_token\": DEFAULT_UNK_TOKEN,\n",
        "            }\n",
        "        )    \n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "\n",
        "\n",
        "    elif args.model == 'bloom':\n",
        "        actor = BLOOMActor(pretrained=args.pretrain, lora_rank=args.lora_rank).to(torch.cuda.current_device())\n",
        "        critic = BLOOMCritic(pretrained=args.pretrain, lora_rank=args.lora_rank).to(torch.cuda.current_device())\n",
        "        tokenizer = BloomTokenizerFast.from_pretrained(args.pretrain)\n",
        "        tokenizer.pad_token = tokenizer.eos_token            \n",
        "    elif args.model == 'opt':\n",
        "        actor = OPTActor(pretrained=args.pretrain, lora_rank=args.lora_rank).to(torch.cuda.current_device())\n",
        "        critic = OPTCritic(pretrained=args.pretrain, lora_rank=args.lora_rank).to(torch.cuda.current_device())\n",
        "        tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")            \n",
        "    else:\n",
        "        raise ValueError(f'Unsupported model \"{args.model}\"')\n",
        "\n",
        "    initial_model = deepcopy(actor)\n",
        "    reward_model = RewardModel(deepcopy(critic.model), deepcopy(critic.value_head)).to(torch.cuda.current_device())"
      ],
      "metadata": {
        "id": "Og7ueBVvPPtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# configure optimizer\n",
        "if args.strategy.startswith('colossalai'):\n",
        "    actor_optim = HybridAdam(actor.parameters(), lr=5e-6)\n",
        "    critic_optim = HybridAdam(critic.parameters(), lr=5e-6)\n",
        "else:\n",
        "    actor_optim = Adam(actor.parameters(), lr=5e-6)\n",
        "    critic_optim = Adam(critic.parameters(), lr=5e-6)"
      ],
      "metadata": {
        "id": "9DhMVvYjPR2n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# setting the models\n",
        "(actor, actor_optim), (critic, critic_optim), reward_model, initial_model = strategy.prepare(\n",
        "    (actor, actor_optim), (critic, critic_optim), reward_model, initial_model)"
      ],
      "metadata": {
        "id": "Sp_dgLebPTe3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare data\n",
        "with open(args.data_path_3_PPO, \"r\", encoding='utf-8-sig') as json_file:\n",
        "    list_data_dict = json.load(json_file)\n",
        "    list_prompt = [tmp['prompt'] for tmp in list_data_dict]\n",
        "\n",
        "def tokenize_fn(texts):\n",
        "    batch = tokenizer(texts, return_tensors='pt', max_length=96, padding=True, truncation=True)\n",
        "    return {k: v.cuda() for k, v in batch.items()}\n",
        "\n",
        "print(list_prompt)\n",
        "print('\\n\\n\\n')\n",
        "print(tokenize_fn('I want you to act as a linux terminal.'))"
      ],
      "metadata": {
        "id": "ThJWdu2aPVD-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# configure trainer\n",
        "trainer = PPOTrainer(strategy,\n",
        "                     actor,\n",
        "                     critic,\n",
        "                     reward_model,\n",
        "                     initial_model,\n",
        "                     actor_optim,\n",
        "                     critic_optim,\n",
        "                     max_epochs=args.max_epochs,\n",
        "                     train_batch_size=args.train_batch_size,\n",
        "                     tokenizer=tokenize_fn,\n",
        "                     max_length=128,\n",
        "                     do_sample=True,\n",
        "                     temperature=1.0,\n",
        "                     top_k=50,\n",
        "                     pad_token_id=tokenizer.pad_token_id,\n",
        "                     eos_token_id=tokenizer.eos_token_id)\n",
        "\n",
        "## train!\n",
        "trainer.fit(list_prompt,  # 입력 prompt\n",
        "            num_episodes=args.num_episodes,\n",
        "            max_timesteps=args.max_timesteps,\n",
        "            update_timesteps=args.update_timesteps)\n",
        "\n",
        "## save\n",
        "# save model checkpoint after fitting on only rank0\n",
        "strategy.save_model(actor, os.path.join(args.output_dir, 'actor.pt'), only_rank0=True)\n",
        "# save optimizer checkpoint on all ranks\n",
        "strategy.save_optimizer(actor_optim,\n",
        "                        os.path.join(args.output_dir, 'actor_optim_checkpoint_%d.pt' % (torch.cuda.current_device())),\n",
        "                        only_rank0=False)"
      ],
      "metadata": {
        "id": "mctryMQGPV6y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8ydF3RbQE0Uy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "actor=GPTActor(pretrained=args.pretrain_actor, lora_rank=args.lora_rank)"
      ],
      "metadata": {
        "id": "qXGgCwz5KH01"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## inference\n",
        "def generation(input_text):\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors='pt').to(\n",
        "        torch.cuda.current_device())\n",
        "    outputs = actor.generate(input_ids,\n",
        "                             max_length=args.max_length,\n",
        "                             do_sample=True,\n",
        "                             top_k=50,\n",
        "                             top_p=0.95,\n",
        "                             num_return_sequences=1)\n",
        "    output = tokenizer.batch_decode(outputs[0], skip_special_tokens=True)[0]\n",
        "    print(\"-\"*70)\n",
        "    print((\"챗봇 : {}\").format(output))\n",
        "    print(\"-\"*70)\n",
        "    return output"
      ],
      "metadata": {
        "id": "1ahSVzSxPZdS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list_prompt = [\"인공지능은 무엇인가 설명해줘\",\"하늘에는 왜 구름이 있을까?\"]\n",
        "\n",
        "for input_text in list_prompt:\n",
        "  print(\"-\"*70)\n",
        "  print((\"질문 : {}\").format(input_text))\n",
        "  output = generation(input_text)"
      ],
      "metadata": {
        "id": "2ldehpLuJEk_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5QA-sQ0ig5M2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h16qOhQug5J_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##학습 데이터 활용하기"
      ],
      "metadata": {
        "id": "vdUS_YHlhXeu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import\n",
        "import argparse\n",
        "\n",
        "import torch\n",
        "torch.cuda.empty_cache()\n",
        "from chatgpt.models.bloom import BLOOMActor\n",
        "from chatgpt.models.gpt import GPTActor\n",
        "from chatgpt.models.opt import OPTActor\n",
        "from transformers import AutoTokenizer\n",
        "from transformers.models.gpt2.tokenization_gpt2 import GPT2Tokenizer\n",
        "\n",
        "# data config\n",
        "IGNORE_INDEX = -100\n",
        "DEFAULT_PAD_TOKEN = \"[PAD]\"\n",
        "DEFAULT_EOS_TOKEN = \"</s>\"\n",
        "DEFAULT_BOS_TOKEN = \"</s>\"\n",
        "DEFAULT_UNK_TOKEN = \"</s>\"\n",
        "PROMPT_DICT = {\n",
        "    \"prompt_input\": (\n",
        "        \"Instruction(명령어):{prompt}\\n\\n### Input(입력):\\n{input}\\n\\nResponse(응답):\"\n",
        "    ),\n",
        "    \"prompt_no_input\": (\n",
        "        \"Instruction(명령어):{prompt}\\n\\nResponse(응답):\"\n",
        "    ),\n",
        "}"
      ],
      "metadata": {
        "id": "XgXMqdh5g5Hu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# define argment\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--model',\n",
        "                    default='gpt2',\n",
        "                    choices=['gpt2', 'bloom', 'opt'])\n",
        "# We suggest to use the pretrained model from HuggingFace, use pretrain to configure model\n",
        "parser.add_argument('--pretrain', type=str, default=None)\n",
        "parser.add_argument('--model_path', type=str, default=None)\n",
        "parser.add_argument('--input',\n",
        "                    type=str,\n",
        "                    default='Question: How are you ? Answer:')\n",
        "parser.add_argument('--max_length', type=int, default=250)\n",
        "args_inference = parser.parse_args([])\n",
        "\n",
        "args_inference.model = 'gpt2'\n",
        "args_inference.pretrain = 'skt/kogpt2-base-v2'\n",
        "args_inference.model_directory = '/content/drive/MyDrive/인공지능강의/output_3_PPO'\n",
        "args_inference.model_path = os.path.join(args_inference.model_directory, 'actor.pt')\n",
        "\n",
        "# configure model, tokenizer\n",
        "if args_inference.model == 'gpt2':\n",
        "    #actor = GPTActor(pretrained=args_inference.pretrain).to(torch.cuda.current_device())\n",
        "    actor = GPTActor(pretrained=args_inference.pretrain)\n",
        "    # tokenizer = GPT2Tokenizer.from_pretrained(args_inference.pretrain)\n",
        "    # tokenizer.pad_token = tokenizer.eos_token\n",
        "    tokenizer = AutoTokenizer.from_pretrained(args_inference.pretrain,\n",
        "                                              padding_side=\"right\",\n",
        "                                              model_max_length=512)\n",
        "    tokenizer.add_special_tokens({\n",
        "        \"eos_token\": DEFAULT_EOS_TOKEN,\n",
        "        \"bos_token\": DEFAULT_BOS_TOKEN,\n",
        "        \"unk_token\": DEFAULT_UNK_TOKEN,\n",
        "    })\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "elif args_inference.model == 'bloom':\n",
        "    actor = BLOOMActor(pretrained=args_inference.pretrain).to(\n",
        "        torch.cuda.current_device())\n",
        "    tokenizer = AutoTokenizer.from_pretrained('bigscience/bloom-560m')\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "elif args_inference.model == 'opt':\n",
        "    actor = OPTActor(pretrained=args_inference.pretrain).to(torch.cuda.current_device())\n",
        "    tokenizer = AutoTokenizer.from_pretrained('facebook/opt-350m')\n",
        "else:\n",
        "    raise ValueError(f'Unsupported model \"{args_inference.model}\"')\n",
        "\n",
        "state_dict = torch.load(args_inference.model_path, map_location='cpu');\n",
        "actor.model.load_state_dict(state_dict);\n",
        "\n",
        "actor.eval();"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ynju55ZIg7_t",
        "outputId": "cf77da63-1b18-4b82-a1c9-16a625a8b64d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## inference\n",
        "def generation(input_text):\n",
        "    #input_ids = tokenizer.encode(input_text, return_tensors='pt').to(torch.cuda.current_device())\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
        "    outputs = actor.generate(input_ids,\n",
        "                             max_length=args_inference.max_length,\n",
        "                             do_sample=True,\n",
        "                             top_k=50,\n",
        "                             top_p=0.95,\n",
        "                             num_return_sequences=1)\n",
        "    output = tokenizer.batch_decode(outputs[0], skip_special_tokens=True)[0]\n",
        "    print(\"-\"*70)\n",
        "    print((\"챗봇 : {}\").format(output))\n",
        "    print(\"-\"*70)\n",
        "    return output"
      ],
      "metadata": {
        "id": "1XGTiOOJhLaG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list_prompt = [\"인공지능은 무엇인가 설명해줘\",\"하늘에는 왜 구름이 있을까?\"]\n",
        "\n",
        "for input_text in list_prompt:\n",
        "  print(\"-\"*70)\n",
        "  print((\"질문 : {}\").format(input_text))\n",
        "  output = generation(input_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5P4Qp1hqhNPW",
        "outputId": "2f0fbabf-82b3-4db2-d6fb-bf49f10bfebf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------------\n",
            "질문 : 인공지능은 무엇인가 설명해줘\n",
            "----------------------------------------------------------------------\n",
            "챗봇 : 인공지능은 무엇인가 설명해줘요.\n",
            "어머님이 보시는 게 바로 그거예요?\n",
            "어머님은 그게 뭔지 아세요?\n",
            "아니 그냥 그~ 알 수가 없는 건데 왜 그~ 그~ 그~ 봇물 터지는 그~ 영상을 보고 진짜 어머님은 얼마나 이쁘게 보셨을까라고 생각하시겠지만 그~ 그~ 그~ 그~ 어머님이 이게 무슨 소릴 하시는지 몰랐지만 이제 그~ 뭐랄까 되게 궁금하잖아요.\n",
            "그~ 그~ 그~ 아~ 그~ 그~ 그~ 그~ 니까?\n",
            "아~ 그래서 그~ 아~ 제가 그~ 지금 그~ 니까?\n",
            "그래서 그~ 그~ 니까?\n",
            "그~ 아~ 아니 근데 아니 그~ 니까?\n",
            "그~ 그~ 아~ 그럼 어~ 뭐야 아~ 그~ 아~ 어~ 이~ 그~ 뭔지 궁금하잖아요.\n",
            "그~ 니?\n",
            "그~ 그~ 니까?\n",
            "그~ 아~ 뭔지 궁금한 거예요?\n",
            "아니 니까?\n",
            "니까?\n",
            "아니 뭐야 그래서 니까?\n",
            "아니 그~ 저희 아~ 그~\n",
            "----------------------------------------------------------------------\n",
            "----------------------------------------------------------------------\n",
            "질문 : 하늘에는 왜 구름이 있을까?\n",
            "----------------------------------------------------------------------\n",
            "챗봇 : 하늘에는 왜 구름이 있을까? 어때? 지금이 땡땡이이야. 우리 집이 다 좋았지.\"\n",
            "그 말에 놀란 엄마는 말했다.\n",
            "\"나는 다들 좋았어. 난 엄마한테 잘됐어. 너희들 다 잘됐어. 내 아들도 엄마가 잘됐으면 좋겠다고. 우리 집 사람들이 잘돼서 좋았어.\"\n",
            "\"그렇다면 이제 집으로 돌아가도 돼?\"\n",
            "\"내가 엄마한테 잘됐으면 좋겠어.\"\n",
            "\"그럼, 우리 집 사람들에게 잘될 거야. 우리 집 사람들은 다 잘됐어. 내 아들도 잘됐으면 좋겠어.\"\n",
            "\"이렇게 말했거든. 내가 엄마한테 잘됐으면 좋겠어. 내 아들도 잘됐으면 좋겠어.\"\n",
            "\"엄마. 우리 집 사람들 다 잘됐으면 좋겠어. 엄마는 땡땡이도 잘됐으면 좋겠어.\"\n",
            "\"그래. 우리 집 사람들에게 잘됐어. 우리 동네 사람들은 다 잘됐으면 좋겠어.\"\n",
            "\"그럼, 엄마. 이게 엄마가 잘됐으면 좋겠어. 엄마가 땡땡이를 많이 잘했으면 좋겠어. 엄마는 다 잘됐으면 좋겠어.\"\n",
            "\"그럼 내가 너\n",
            "----------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "sKFjohc4OGLk"
      }
    }
  ]
}